{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "**Other**\n",
    "* CPU monitoring in terminal:  \n",
    "```bash\n",
    "top\n",
    "```\n",
    "* GPU monitoring in terminal:  \n",
    "```bash\n",
    "pip install gpustat\n",
    "watch -c gpustat -cp --color\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "# to save results to data directory\n",
    "module_path = '..'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(1, module_path)\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm_optimizer as lgbmo\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import lime\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from eli5 import explain_weights, explain_weights_df,explain_prediction_df\n",
    "\n",
    "from pytorch_widedeep.utils import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, RobustScaler, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import importlib\n",
    "\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "# temporarily remove deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intsec(list1, list2):\n",
    "    \"\"\"Simple intesection of two lists.\n",
    "\n",
    "    Args:\n",
    "        list1 (list): list1\n",
    "        list2 (list): list2\n",
    "\n",
    "    Returns:\n",
    "        list (list): intersection of lists\n",
    "    \"\"\"\n",
    "    return list(set.intersection(set(list1), set(list2)))\n",
    "\n",
    "\n",
    "def scale(data_pd, non_scale_cols, scaler_sk='Standard'):\n",
    "    \"\"\"Procedure to scale the dataset except the given list of columns.\n",
    "\n",
    "    Args:\n",
    "        data_pd (obj): pandas dataframe\n",
    "        non_scale_cols (list): columns to not scale\n",
    "        scaler_sk (str, sklearn.peprocessing obj): type of scaler from['Standard', 'Yeo-Johnson',\n",
    "        'Robust', 'MinMax'] or already fitted scaler\n",
    "\n",
    "    Returns:\n",
    "        tuple (tuple): data_pd_scaled (obj): scaled pandas dataframe\\n\n",
    "        scaler_sk (obj): sklearn scaler object\n",
    "    \"\"\"\n",
    "    non_scale_cols = intsec(data_pd.columns.values, non_scale_cols)\n",
    "    data_pd_toscale = data_pd.drop(columns=non_scale_cols)\n",
    "    if type(scaler_sk) is str:\n",
    "        if scaler_sk == 'Standard':\n",
    "            scaler_sk = StandardScaler()\n",
    "        elif scaler_sk == 'Yeo-Johnson':\n",
    "            scaler_sk = PowerTransformer(method='yeo-johnson')\n",
    "        elif scaler_sk == 'Robust':\n",
    "            scaler_sk = RobustScaler()\n",
    "        elif scaler_sk == 'MinMax':\n",
    "            scaler_sk = MinMaxScaler()\n",
    "        scaler_sk.fit(data_pd_toscale)\n",
    "    # if 'sklearn.peprocessing' in str(type(scaler_sk)):\n",
    "\n",
    "    data_pd_scaled = pd.DataFrame(scaler_sk.transform(data_pd_toscale),\n",
    "                                  columns=data_pd_toscale.columns.values)\n",
    "    data_pd_scaled[non_scale_cols] = data_pd[non_scale_cols].copy()\n",
    "    return data_pd_scaled, scaler_sk\n",
    "\n",
    "\n",
    "def rmse(y_true, ypred):\n",
    "    return mean_squared_error(y_true, ypred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set (i)identifier and which columns are (ii)numerical and (iii)categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unclosed file <_io.TextIOWrapper name='../data/column_types_im.json' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "column_types = json.load(open('./#datasets/column_types.json', 'r'))\n",
    "\n",
    "identifier = column_types['identifier']\n",
    "cat_cols = column_types['categorical']\n",
    "\n",
    "target = column_types['target']\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle('./#datasets/data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill NA - 0 for numerical and 'NA' for categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "data_raw[cat_cols] = data_raw[cat_cols].fillna('NA')\n",
    "data_raw[cat_cols] = data_raw[cat_cols].astype(str)\n",
    "# non-categorical\n",
    "non_cat_cols = data_raw.drop(columns=cat_cols).columns.tolist()\n",
    "data_raw[non_cat_cols] = data_raw[non_cat_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constant columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_cols = data_raw.columns[data_raw.nunique() == 1].values.tolist()\n",
    "data_raw.drop(columns=const_cols, inplace=True)\n",
    "print('Dropped constant columns:\\n{}'.format(const_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train, valid, test dataset split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = train_test_split(data_raw, test_size=0.2, random_state=1)\n",
    "data_valid, data_test = train_test_split(data_valid, test_size=0.5, random_state=1)\n",
    "\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "data_valid.reset_index(inplace=True, drop=True)\n",
    "data_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_filtered = intsec(cat_cols, data_raw.columns.values)\n",
    "data_train_scaled, Scaler = scale(data_train, cat_cols_filtered  + [target], scaler_sk='Standard')\n",
    "data_valid_scaled, Scaler = scale(data_train, cat_cols_filtered  + [target], scaler_sk=Scaler)\n",
    "data_test_scaled, Scaler = scale(data_train, cat_cols_filtered  + [target], scaler_sk=Scaler)\n",
    "\n",
    "Ptran = PowerTransformer(standardize = False)\n",
    "Ptran.fit(data_train_scaled.loc[data_train_scaled[target] > 0, target].values.reshape(-1, 1))\n",
    "\n",
    "data_train_scaled.loc[data_train_scaled[target] > 0, target] = Ptran.transform(data_train_scaled.loc[data_train_scaled[target] > 0, target].values.reshape(-1, 1)).flatten()\n",
    "data_valid_scaled.loc[data_valid_scaled[target] > 0, target] = Ptran.transform(data_valid_scaled.loc[data_valid_scaled[target] > 0, target].values.reshape(-1, 1)).flatten()\n",
    "# no need to power-transform test target\n",
    "#data_test_scaled.loc[data_train_scaled[target] > 0, target] = Ptran.transform(data_train_scaled. loc[data_train_scaled[target] > 0,target].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label encode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder(cat_cols_filtered)\n",
    "label_encoder.fit(data_raw[cat_cols_filtered])\n",
    "\n",
    "data_train_scaled[cat_cols_filtered] = label_encoder.transform(data_train_scaled[cat_cols_filtered])\n",
    "data_valid_scaled[cat_cols_filtered] = label_encoder.transform(data_valid_scaled[cat_cols_filtered])\n",
    "data_test_scaled[cat_cols_filtered] = label_encoder.transform(data_test_scaled[cat_cols_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "* based on \n",
    "  * [Putting ML in production II: logging and monitoring](https://towardsdatascience.com/putting-ml-in-production-ii-logging-and-monitoring-algorithms-91f174044e4e)\n",
    "  * [10.1 Regression with GBMs: preparing the data](https://github.com/jrzaurin/RecoTour/blob/master/Ponpare/Chapter10_GBM_reg_Recommendations.ipynb)\n",
    "  * [ml-pipeline](https://github.com/jrzaurin/ml-pipeline)\n",
    "  * [tabulardl-benchmark](https://github.com/jrzaurin/tabulardl-benchmark/blob/master/run_experiments/adult/adult_lightgbm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPTIMIZE_WITH = \"hyperopt\"\n",
    "RESULTS_DIR = \"./\"\n",
    "MODELS_DIR = \"./\"\n",
    "suffix = str(datetime.now()).replace(\" \", \"_\").split(\".\")[:-1][0]\n",
    "results_filename = \"_\".join([\"lightgbm_results\", suffix]) + \".pkl\"\n",
    "models_filename = \"_\".join([\"lightgbm_model\", suffix]) + \".pkl\"\n",
    "\n",
    "lgbtrain = lgb.Dataset(data_train_scaled.drop(columns=[target]), data_train_scaled[target], categorical_feature=cat_cols_filtered, free_raw_data=False)\n",
    "lgbvalid = lgb.Dataset(data_valid_scaled.drop(columns=[target]), data_valid_scaled[target], reference=lgbtrain, free_raw_data=False)\n",
    "\n",
    "# https://lightgbm.readthedocs.io/en/latest/Parameters.html?highlight=rmse#core-parameters\n",
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "# https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html#\n",
    "if OPTIMIZE_WITH == \"optuna\":\n",
    "    optimizer = lgbmo.LGBOptimizerOptuna(objective='regression', metric='rmse', verbose=True)\n",
    "elif OPTIMIZE_WITH == \"hyperopt\":\n",
    "    optimizer = lgbmo.LGBOptimizerHyperopt(objective='regression', metric=rmse, verbose=True)\n",
    "\n",
    "optimizer.optimize(lgbtrain, lgbvalid)\n",
    "\n",
    "# Final TRAIN/TEST\n",
    "ftrain = pd.concat([data_train_scaled, data_valid_scaled]).reset_index(drop=True)\n",
    "flgbtrain = lgb.Dataset(ftrain.drop(columns=[target]), ftrain[target], categorical_feature=cat_cols_filtered, free_raw_data=False)\n",
    "lgbtest = lgb.Dataset(data_test_scaled.drop(columns=[target]), data_test_scaled[target], categorical_feature=cat_cols_filtered, reference=flgbtrain, free_raw_data=False)\n",
    "\n",
    "params = copy(optimizer.best)\n",
    "params[\"n_estimators\"] = 1000\n",
    "\n",
    "\n",
    "start = time()\n",
    "model = lgb.train(params, flgbtrain, valid_sets=[lgbtest], early_stopping_rounds=50, verbose_eval=True)\n",
    "runtime = time() - start\n",
    "\n",
    "data_pred = (model.predict(lgbtest.data))\n",
    "data_pred = Ptran.inverse_transform(data_pred.reshape(-1, 1))\n",
    "rmse_test_score = mean_squared_error(data_test_scaled[target], data_pred, squared=False)\n",
    "\n",
    "# SAVE\n",
    "# results_d = {}\n",
    "# results_d[\"best_params\"] = optimizer.best\n",
    "# results_d[\"runtime\"] = runtime\n",
    "# results_d[\"rmse\"] = rmse_test_score_reg02\n",
    "\n",
    "# with open(RESULTS_DIR + results_filename, \"wb\") as f:\n",
    "#     pickle.dump(results_d, f)\n",
    "\n",
    "# with open(MODELS_DIR + model_filename, \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2356612039261006"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_test_score"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9d18e9f29cc76176cc64cecda10b08503df27bbeb68d46276fb666f16272d10"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}