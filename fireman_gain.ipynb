{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlad GAIN\n",
    "!git clone git@github.com:jsyoon0823/GAIN.git\n",
    "# upgrade GAIN to tensorflow v2, tf.placeholder is no longer working in v2 etc.\n",
    "!tf_upgrade_v2 --infile GAIN/gain.py --outfile GAIN/gain_tf2.py\n",
    "!tf_upgrade_v2 --infile GAIN/utils.py --outfile GAIN/utils_tf2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename utils to .utils_tf2 in GAIN/gain_tf2.py\n",
    "#from utils import normalization, renormalization, rounding\n",
    "#from utils import xavier_init\n",
    "#from utils import binary_sampler, uniform_sampler, sample_batch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from GAIN.gain_tf2 import gain\n",
    "from GAIN.utils_tf2 import binary_sampler\n",
    "from GAIN.utils_tf2 import rmse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create missing data in TEP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Tennessee_Event-Driven/datasets/dataset.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_X = dataset.drop(columns='fault_id').values\n",
    "dataset_Y = dataset['fault_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no, dim = dataset_X.shape\n",
    "p = 0.1\n",
    "# Introduce missing data\n",
    "mask = binary_sampler(1-p, no, dim)\n",
    "dataset_X_missing = dataset_X.copy()\n",
    "dataset_X_missing[mask == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "p = 0.1\n",
    "mask = np.random.choice(a=[True, False], size=(dataset_X.shape[0], dataset_X.shape[1]), p=[p, 1-p])\n",
    "# just a check if the mask really does what I want..\n",
    "#len(dataset_X[np.where(mask == True)])/(dataset_X.shape[1] * dataset_X.shape[0])\n",
    "dataset_X_missing = np.copy(dataset_X)\n",
    "dataset_X_missing[mask] = np.nan\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_parameters = {'batch_size': 128,\n",
    "                 'hint_rate': 1.5,\n",
    "                 'alpha': 100,\n",
    "                 'iterations': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 168.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_X_imputed = gain(dataset_X_missing, gain_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07301131265367386"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_X_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIN imputation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Tennessee_Event-Driven/datasets/dataset.csv',index_col=False)\n",
    "\n",
    "dataset_X = dataset.drop(columns='fault_id').values\n",
    "dataset_Y = dataset['fault_id'].values\n",
    "\n",
    "no, dim = dataset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 27.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 225.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 24.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 218.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 22.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 209.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 219.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 19.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 194.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 18.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 200.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 17.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 190.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 16.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 179.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 15.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 205.72it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 13.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 200.12it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 13.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 182.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 181.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 152.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.13it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 174.46it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:06<00:00, 166.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  9.98it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 179.77it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size_list = [16,32]#,64,128,256]\n",
    "hint_rate_list = [1,2]#,5,10,100]\n",
    "alpha_list = [1,10]#,100,1000,10000]\n",
    "iterations_list = [10,100]#,1000,10000,100000]\n",
    "p_list = [0.05,0.1]#,0.15,0.2,0.25]\n",
    "\n",
    "\n",
    "dataset_X_rmse_list = []\n",
    "\n",
    "for p in p_list:\n",
    "    mask = binary_sampler(1-p, no, dim)\n",
    "    dataset_X_missing = dataset_X.copy()\n",
    "    dataset_X_missing[mask == 0] = np.nan\n",
    "    dataset_X_rmse_list_temp = []\n",
    "    index_names = []\n",
    "\n",
    "    for batch_size in batch_size_list:\n",
    "        for hint_rate in hint_rate_list:\n",
    "            for alpha in alpha_list:\n",
    "                for iterations in iterations_list:\n",
    "                    gain_parameters = {'batch_size': batch_size,\n",
    "                                     'hint_rate': hint_rate,\n",
    "                                     'alpha': alpha,\n",
    "                                     'iterations': iterations}\n",
    "                    dataset_X_imputed = gain(dataset_X_missing, gain_parameters)\n",
    "                    dataset_X_rmse_list_temp.append(rmse_loss(dataset_X, dataset_X_imputed, mask))\n",
    "                    index_names.append('batch'+str(batch_size)+'_hint_rate'+str(hint_rate)+'_alpha'+str(alpha)+'_iter'+str(iterations))\n",
    "    dataset_X_rmse_list.append(dataset_X_rmse_list_temp)\n",
    "\n",
    "result_pd = pd.DataFrame(index = index_names, columns=[str(p) for p in p_list], data = np.array(dataset_X_rmse_list).T)\n",
    "result_pd.to_csv('TEP_GAINImputation_RMSE.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate1_alpha1_iter10</th>\n",
       "      <td>0.176357</td>\n",
       "      <td>0.170159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate1_alpha1_iter1000</th>\n",
       "      <td>0.081187</td>\n",
       "      <td>0.079137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate1_alpha10_iter10</th>\n",
       "      <td>0.190125</td>\n",
       "      <td>0.170045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate1_alpha10_iter1000</th>\n",
       "      <td>0.080075</td>\n",
       "      <td>0.081688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate2_alpha1_iter10</th>\n",
       "      <td>0.176881</td>\n",
       "      <td>0.202493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate2_alpha1_iter1000</th>\n",
       "      <td>0.080355</td>\n",
       "      <td>0.086534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate2_alpha10_iter10</th>\n",
       "      <td>0.185521</td>\n",
       "      <td>0.188545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch16_hint_rate2_alpha10_iter1000</th>\n",
       "      <td>0.081752</td>\n",
       "      <td>0.082289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate1_alpha1_iter10</th>\n",
       "      <td>0.176655</td>\n",
       "      <td>0.179522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate1_alpha1_iter1000</th>\n",
       "      <td>0.078439</td>\n",
       "      <td>0.090730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate1_alpha10_iter10</th>\n",
       "      <td>0.202787</td>\n",
       "      <td>0.161296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate1_alpha10_iter1000</th>\n",
       "      <td>0.077408</td>\n",
       "      <td>0.076167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate2_alpha1_iter10</th>\n",
       "      <td>0.179796</td>\n",
       "      <td>0.199158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate2_alpha1_iter1000</th>\n",
       "      <td>0.078240</td>\n",
       "      <td>0.082484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate2_alpha10_iter10</th>\n",
       "      <td>0.212516</td>\n",
       "      <td>0.201565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch32_hint_rate2_alpha10_iter1000</th>\n",
       "      <td>0.077969</td>\n",
       "      <td>0.077884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0.05       0.1\n",
       "batch16_hint_rate1_alpha1_iter10     0.176357  0.170159\n",
       "batch16_hint_rate1_alpha1_iter1000   0.081187  0.079137\n",
       "batch16_hint_rate1_alpha10_iter10    0.190125  0.170045\n",
       "batch16_hint_rate1_alpha10_iter1000  0.080075  0.081688\n",
       "batch16_hint_rate2_alpha1_iter10     0.176881  0.202493\n",
       "batch16_hint_rate2_alpha1_iter1000   0.080355  0.086534\n",
       "batch16_hint_rate2_alpha10_iter10    0.185521  0.188545\n",
       "batch16_hint_rate2_alpha10_iter1000  0.081752  0.082289\n",
       "batch32_hint_rate1_alpha1_iter10     0.176655  0.179522\n",
       "batch32_hint_rate1_alpha1_iter1000   0.078439  0.090730\n",
       "batch32_hint_rate1_alpha10_iter10    0.202787  0.161296\n",
       "batch32_hint_rate1_alpha10_iter1000  0.077408  0.076167\n",
       "batch32_hint_rate2_alpha1_iter10     0.179796  0.199158\n",
       "batch32_hint_rate2_alpha1_iter1000   0.078240  0.082484\n",
       "batch32_hint_rate2_alpha10_iter10    0.212516  0.201565\n",
       "batch32_hint_rate2_alpha10_iter1000  0.077969  0.077884"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(index = index_names, columns=[str(p) for p in p_list], data = np.array(dataset_X_rmse_list).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest classification parameter tuning - GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'n_estimators': [100, 200, 500],\n",
    "               'max_features': ['auto', 'log2'],\n",
    "               'max_depth' : [5,10,50,100,None],\n",
    "               'criterion' :['gini', 'entropy']}]\n",
    "\n",
    "RF_clf_gs = GridSearchCV(estimator = RandomForestClassifier(), param_grid=param_grid, scoring='f1',n_jobs=4, cv=10)\n",
    "# afterwards change dataset_X to dataset_X_imputed\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset_X)\n",
    "scaled_dataset_X = scaler.transform(dataset_X) \n",
    "\n",
    "RF_clf_gs.fit(scaled_dataset_X, dataset_Y)\n",
    "means = RF_clf_gs.cv_results_['mean_test_score']\n",
    "stds = RF_clf_gs.cv_results_['std_test_score']\n",
    "print('RF 10CV f1 score mean with 95% confidence interval : ')\n",
    "for mean, std, params in zip(means, stds, RF_clf_gs.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _impute(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _scale(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _classify(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _split_data(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def process(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class FIREMAN_Pipeline(Pipeline):\n",
    "    def __init__(self, dataset_x, dataset_y, imputer='Simple', scaler='RandomScaler', classifier='RandomForest', scorer='report'):\n",
    "        self.dataset_x = dataset_x\n",
    "        self.dataset_y = dataset_y\n",
    "        self.imputer = imputer\n",
    "        self.scaler = scaler\n",
    "        self.classifier = classifier\n",
    "        self.scorer = scorer\n",
    "\n",
    "    def _impute(self):\n",
    "        if self.imputer == 'GAIN':\n",
    "            pass\n",
    "        \n",
    "        elif self.imputer == 'Simple':\n",
    "            imputer = SimpleImputer()\n",
    "            imputed_x = imputer.fit_transform(self.dataset_x)\n",
    "        \n",
    "        elif self.imputer == '':\n",
    "            imputed_x = self.dataset_x\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        \n",
    "        return imputed_x\n",
    "    \n",
    "    def _scale(self, x):\n",
    "        if self.scaler == 'RandomScaler':\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(x)\n",
    "            return scaler.transform(x) \n",
    "\n",
    "        elif self.scaler == '':\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def _split_data(self, x):\n",
    "        if self.scorer == 'cv_score':\n",
    "            return x, self.dataset_y\n",
    "\n",
    "        elif self.scorer == 'report':\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, self.dataset_y, test_size=0.1)\n",
    "            return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    def _classify(self):\n",
    "        if self.classifier == 'RandomForest':\n",
    "            self.classifier = RandomForestClassifier()\n",
    "        else:\n",
    "            raise NotImplementedError()     \n",
    "\n",
    "    def process(self):\n",
    "        x_missing = self._impute()\n",
    "        x_scaled = self._scale(x_missing)\n",
    "        if self.scorer=='report':\n",
    "            x_train, x_test, y_train, y_test = self._split_data(x_scaled)\n",
    "            self._classify()\n",
    "            self.classifier.fit(x_train, y_train)\n",
    "            y_predicted = self.classifier.predict(x_test)\n",
    "            return print(classification_report(y_test, y_predicted))\n",
    "        \n",
    "        elif self.scorer=='cv_score':\n",
    "            x, y = self._split_data(dataset_x_scaled)\n",
    "            _classify()\n",
    "            return print(cross_val_score(self.classifier, x, y, cv=10, scoring='f1_weighted'))\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Tennessee_Event-Driven/datasets/dataset.csv',index_col=False)\n",
    "\n",
    "dataset_X = dataset.drop(columns='fault_id').values\n",
    "dataset_Y = dataset['fault_id'].values\n",
    "\n",
    "no, dim = dataset_X.shape\n",
    "p = 0.1\n",
    "# Introduce missing data\n",
    "mask = binary_sampler(1-p, no, dim)\n",
    "dataset_X_missing = dataset_X.copy()\n",
    "dataset_X_missing[mask == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tep_pipeline = FIREMAN_Pipeline(dataset_X_missing, dataset_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.60      0.42       161\n",
      "           1       1.00      0.89      0.94       151\n",
      "           2       1.00      0.89      0.94       124\n",
      "           3       0.33      0.46      0.38       142\n",
      "           4       0.84      0.75      0.79       141\n",
      "           5       0.72      0.76      0.74       147\n",
      "           6       1.00      0.88      0.93       130\n",
      "           7       1.00      0.86      0.92       149\n",
      "           8       0.99      0.93      0.96       145\n",
      "           9       0.36      0.45      0.40       154\n",
      "          10       0.77      0.69      0.73       147\n",
      "          11       0.80      0.61      0.69       150\n",
      "          12       0.91      0.82      0.86       141\n",
      "          13       1.00      0.83      0.91       156\n",
      "          14       0.97      0.79      0.87       166\n",
      "          15       0.30      0.44      0.35       142\n",
      "          16       0.76      0.65      0.70       160\n",
      "          17       0.82      0.70      0.75       123\n",
      "          18       0.98      0.79      0.88       130\n",
      "          19       0.63      0.66      0.64       134\n",
      "          20       0.82      0.63      0.71       133\n",
      "          21       0.97      0.88      0.92       144\n",
      "\n",
      "    accuracy                           0.72      3170\n",
      "   macro avg       0.79      0.73      0.75      3170\n",
      "weighted avg       0.78      0.72      0.75      3170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tep_pipeline.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
