{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Deep learning topology design notes:\n",
    "* usually input layer is not included in layer notation (eg. 2/8 means hidden layer with 2 nodes and output layer with 8)\n",
    "* 1 hidden layer is enough for [universal approximator](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "    * <sub><sup>¨Specifically, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any “squashing” activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired non-zero amount of error, provided that the network is given enough hidden units.¨\n",
    "    p.198[1]<sub><sup>\n",
    "* 1 hidden layer can be sufficient but it is ineffective\n",
    "    * <sub><sup>\"Since a single sufficiently large hidden layer is adequate for approximation of most functions, why would anyone ever use more? One reason hangs on the words “sufficiently large”. Although a single hidden layer is optimal for some functions, there are others for which a single-hidden-layer-solution is very inefficient compared to solutions with more layers.\"[2]<sub><sup>\n",
    "* depth increses generalization \n",
    "    * <sub><sup>\"Empirically, greater depth does seem to result in better generalization for a wide variety of tasks. […] This suggests that using deep architectures does indeed express a useful prior over the space of functions the model learns.\"[3]<sub><sup>\n",
    "* too few neurons - underfitting, too many - overfitting<sub><sup>[4]<sub><sup>\n",
    "\n",
    "    \n",
    "## 1. Number of Neurons and Layers\n",
    "\n",
    "\n",
    "### 1.1. Number of Neurons\n",
    "| Number of Hidden Layers | Result |\n",
    "| :---                    | :----  |\n",
    "| none                    |Only capable of representing linear separable functions or decisions.\n",
    "| 1                       | Can approximate arbitrarily with any functions which con-tains a continuous mapping from one finite space to another.|\n",
    "| 2                       | Represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy.|\n",
    "<sup><sup>[4]<sup><sup>\n",
    "\n",
    "| Search strategy | Description |\n",
    "| :---            | :----       |\n",
    "| Random          | Try random configurations of layers and nodes per layer. |\n",
    "| Grid            | Try a systematic search across the number of layers and nodes per layer. |\n",
    "| Heuristic       | Try a directed search across configurations such as a genetic algorithm or Bayesian optimization. |\n",
    "| Exhaustive      | Try all combinations of layers and the number of nodes; it might be feasible for small networks and datasets. |\n",
    "<sup><sup>[5]<sup><sup>\n",
    "\n",
    "    \n",
    "### 1.2 Number of Layers\n",
    "| Rules of thumb for number of neurons in hidden layer |    \n",
    "| :--- |\n",
    "| The number of hidden neurons should be between the size of the input layer and the size of the output layer |\n",
    "| The number of hidden neurons should be 2/3 of the input layer size, plus the size of the output layer |\n",
    "| The number of hidden neurons should be less than twice the input layer size |\n",
    "<sub><sup>[4]<sub><sup>\n",
    "\n",
    "\n",
    "    \n",
    "## 2. Hyperparameters    \n",
    "<sup><sup>[8]<sup><sup>\n",
    "* use [Ray Tune](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)\n",
    "    \n",
    "### Learning rate (LR)\n",
    "\n",
    "* Perform a learning rate range test to identify a “large” learning rate.\n",
    "* Using the 1-cycle LR policy with a maximum learning rate determined from an LR range test, set a minimum learning rate as a tenth of the maximum.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "* Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum.\n",
    "* If using the 1-cycle learning rate schedule, it is better to use a cyclical momentum (CM) that starts at this maximum momentum value and decreases with increasing learning rate to a value of 0.8 or 0.85.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "* Use as large batch size as possible to fit your memory then you compare performance of different batch sizes.\n",
    "* Small batch sizes add regularization while large batch sizes add less, so utilize this while balancing the proper amount of regularization.\n",
    "* It is often better to use a larger batch size so a larger learning rate can be used.\n",
    "\n",
    "| name | batch size | description |\n",
    "| :--- | :---       | :---        | \n",
    "| Batch Gradient Descent (BGD) | Size of Training Set | converges slowly with accurate estimates of the error gradient |\n",
    "| Stochastic Gradient Descent (SGD) | 1 | converges fast with noisy estimates of the error gradient | \n",
    "| Mini-Batch Gradient Descent | 1 < Batch Size (b) < Size of Training Set | balance between the robustness of SGD and the efficiency of BGD; most common; additional parameter b | \n",
    "<sup><sup>[6][8]<sup><sup>\n",
    "    \n",
    "### Weight decay\n",
    "\n",
    "* A grid search to determine the proper magnitude but usually does not require more than one significant figure accuracy.\n",
    "* A more complex dataset requires less regularization so test smaller weight decay values, such as 10−4 , 10−5 , 10−6 , 0.\n",
    "* A shallow architecture requires more regularization so test larger weight decay values, such as 10−2 , 10−3 , 10−4 .\n",
    "    \n",
    "    \n",
    "## 3. Prunning \n",
    "* evaluating the weighted connections between the layers,  If the network contains any hidden neurons which contain only zero weighted connections, they can be removed.\n",
    "* connections - determine which connections have the least impact to the effectiveness of the neural network, eg. (i) connections with weight below some threshold, (ii) effectivness of neural net if we remove some connections\n",
    "* neurons - determine which neurons are surrounded by weak connections\n",
    "    * possible slight increase/decrease in accuracy, yo uhave evaluate before and after\n",
    "    * Incremental Pruning - essentially forward trial and error selection, just increase number of neurons, check erorr rate, lower number of neurones with lowest eror rate wins\n",
    "\t\t- eg \"check the current error rate in 1,000 cycle intervals.  If the error does not decrease by a single percentage point, then the search will be abandoned.\"\n",
    "    * Selective Pruning - \"examining the weight matrixes of a previously trained neural network.  The selec-tive training algorithm will then attempt to remove neurons without disrupting the output of the neural network.\"\n",
    "\t* [PyTorch pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#pruning-a-module)\n",
    "\n",
    "\n",
    "### 3.1 Trial and error selection method approaches determining the number of hidden neurons\n",
    "| method   | Description |\n",
    "| :---     | :----       |\n",
    "| forward  | start by 2 neurons, train, evaluate and increase the number as long as it improves |\n",
    "| backward | Start with large number of neurons and remove them until the performance is still acceptable |\n",
    "<sup><sup>[4]<sup><sup>\n",
    "\n",
    "\n",
    "\n",
    "## 4. Model evaluation\n",
    "* (i) split test/train, (ii) k-fold, (iii) fixed random seed<sup><sup>[7]<sup><sup>\n",
    "\n",
    "## 5. Weight initialization\n",
    "* prevent layer activation outputs from exploding/vanishing during training\n",
    "    * loss gradients too large/small to flow backwards -> longer/no convergence of the network\n",
    "    \n",
    "\n",
    "<sub><sup>\n",
    "    [1] Goodfelow, I., Yoshua Bengio, and Aaron Courville. \"Deep Learning (Adaptive Computation and Machine Learning Series).\" (2016): 800  \n",
    "    [2] Reed, Russell, and Robert J. MarksII. Neural smithing: supervised learning in feedforward artificial neural networks. Mit Press, 1999.  \n",
    "    [3] Goodfelow, I., Yoshua Bengio, and Aaron Courville. \"Deep Learning (Adaptive Computation and Machine Learning Series).\" (2016): 800.  \n",
    "    [4] Heaton, Jeff. Introduction to neural networks with Java. Heaton Research, Inc., 2008.  \n",
    "    [5] [How to Configure the Number of Layers and Nodes in a Neural Network](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)  \n",
    "    [6] [Difference between a batch and an Epoch](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)  \n",
    "    [7] [Evaluate skill deep learning models](https://machinelearningmastery.com/evaluate-skill-deep-learning-models/)  \n",
    "    [8] [Hyper-parameter Tuning Techniques in Deep Learning](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8)  \n",
    "    [9] [Weight initialization in neural networks a journey from the basics to Kaiming](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)  \n",
    "<sub><sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder in PyTorch - general approach\n",
    "* [Introduction to Variational AutoEncoders](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n",
    "* [Understanding Variational Autoencoders VAEs](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "* https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py\n",
    "* https://github.com/AissamDjahnine/Autoencoder-Pytorch/blob/master/Autoencoder-Pytorch.ipynb\n",
    "* https://github.com/nathanhubens/Autoencoders/blob/master/Variational%20Autoencoders.ipynb\n",
    "* https://github.com/kvfrans/variational-autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def basic_hidden_layer(input_dim, output_dim):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Linear(input_dim, output_dim),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#     )\n",
    "\n",
    "# def basic_output_layer(input_dim, output_dim):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Linear(input_dim, output_dim),\n",
    "#         nn.ReLU(inplace=True),\n",
    "#     )\n",
    "\n",
    "# class GenNet(nn.Module):\n",
    "#     def __init__(self, input_dim=10, hidden_dim1=784, hidden_dim1=128):\n",
    "#         super(AutoEncoder, self).__init__()\n",
    "#         # Build the neural network\n",
    "#         self.layers = nn.Sequential(\n",
    "#             basic_hidden_layer(z_dim, hidden_dim),\n",
    "#             basic_hidden_layer(hidden_dim, hidden_dim * 2),\n",
    "#         )\n",
    "        \n",
    "#     def _init_weights()\n",
    "    \n",
    "#     def forward(self, input_x):\n",
    "#         return self.layers(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_sampler(p, rows, cols):\n",
    "    '''Sample binary random variables.\n",
    "  \n",
    "    Args:\n",
    "    - p: probability of 1\n",
    "    - rows: the number of rows\n",
    "    - cols: the number of columns\n",
    "    \n",
    "    Returns:\n",
    "    - binary_random_matrix: generated binary random matrix.\n",
    "    '''\n",
    "    unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
    "    binary_random_matrix = 1*(unif_random_matrix < p)\n",
    "    return binary_random_matrix\n",
    "\n",
    "def create_missing_data(data_x, probability):\n",
    "    no, dim = data_x.shape\n",
    "    mask = binary_sampler(1-probability, no, dim)\n",
    "    data_x_missing = data_x.copy()\n",
    "    data_x_missing[mask == 0] = 0\n",
    "    return data_x_missing, mask\n",
    "        \n",
    "def cust_dataloader(data_x_scaled, batch_size, probability=0.1, shuffle=True):\n",
    "    data_x_missing, mask = create_missing_data(data_x, probability)\n",
    "    # added .float() as I was getting expected scalar type Float but found Double (numpy stores as Double? https://discuss.pytorch.org/t/pytorch-why-is-float-needed-here-for-runtimeerror-expected-scalar-type-float-but-found-double/98741)\n",
    "    data_x_missing = torch.Tensor(data_x_missing).float()\n",
    "    mask = torch.Tensor(mask).float()\n",
    "    tensor_data_x_mask = TensorDataset(data_x_missing, mask)\n",
    "    return DataLoader(tensor_data_x_mask, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to use : \n",
    "# NetModel.apply(init_weights)\n",
    "# the weights are initialized automatically by Kaiming He:\n",
    "# https://stackoverflow.com/a/56773737/8147433\n",
    "def init_weights(NetModel):\n",
    "    if type(NetModel) == nn.Linear:\n",
    "        # maybe weight.data?, also maybe gain?\n",
    "        # https://discuss.pytorch.org/t/how-to-fix-define-the-initialization-weights-seed/20156/5\n",
    "        # as we are using relu....\n",
    "        torch.nn.init.xavier_normal_(NetModel.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        # in original GAIN post it is 0 but I have also seen 0.01\n",
    "        NetModel.bias.data.fill_(0)\n",
    "        #torch.nn.init.xavier_normal_(NetModel.bias.data)\n",
    "\n",
    "def get_gain_net_block(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # Build the neural network\n",
    "        self.net = nn.Sequential(\n",
    "            get_gain_net_block(input_dim*2, hidden_dim),\n",
    "            get_gain_net_block(hidden_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, data_w_noise, mask):\n",
    "        '''\n",
    "        Function for completing a forward pass of the GainNet\n",
    "        '''\n",
    "        input_data = torch.cat(tensors=[data_w_noise, mask], dim = 1).float()\n",
    "        return self.net(input_data)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Build the neural network\n",
    "        self.net = nn.Sequential(\n",
    "            get_gain_net_block(input_dim*2, hidden_dim),\n",
    "            get_gain_net_block(hidden_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, imputed_data, hint_matrix):\n",
    "        '''\n",
    "        Function for completing a forward pass of the GainNet\n",
    "        '''\n",
    "        input_data = torch.cat(tensors=[imputed_data, hint_matrix], dim = 1).float()\n",
    "        return self.net(input_data)\n",
    "\n",
    "# per https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\n",
    "disc_criterion = torch.nn.BCELoss(reduction = 'mean')\n",
    "\n",
    "def discriminator_loss(gen, disc, disc_criterion, mask, data_w_noise, hint_matrix):\n",
    "    # Generator\n",
    "    # from Coursera GAN lectures/notebooks:\n",
    "    # Since the generator is needed when calculating the discriminator's loss, you will need to \n",
    "    # call .detach() on the generator result to ensure that only the discriminator is updated!\n",
    "    # related to: https://stackoverflow.com/a/58699937/8147433\n",
    "    generator_output = gen(data_w_noise, mask).detach()\n",
    "    # Combine with original data\n",
    "    imputed_data = data_w_noise * mask + generator_output * (1-mask)\n",
    "    # Discriminator\n",
    "    D_prob = disc(imputed_data, hint_matrix)\n",
    "    # Loss\n",
    "    D_loss = disc_criterion(D_prob, mask)\n",
    "    #D_loss = -torch.mean(mask * torch.log(D_prob + 1e-8) + (1-mask) * torch.log(1. - D_prob + 1e-8))\n",
    "    return D_loss\n",
    "\n",
    "gen_criterion = torch.nn.MSELoss(reduction = 'mean')\n",
    "def generator_loss(gen, disc, gen_criterion, data, mask, data_w_noise, hint_matrix):\n",
    "    # Generator\n",
    "    generator_output = gen(data_w_noise, mask)\n",
    "    # Combine with original data\n",
    "    imputed_data = data_w_noise * mask + generator_output * (1-mask)\n",
    "    # Discriminator\n",
    "    D_prob = disc(imputed_data, hint_matrix)\n",
    "    # Loss\n",
    "    G_loss1 = -torch.mean((1-mask) * torch.log(D_prob + 1e-8))\n",
    "    MSE_train_loss = gen_criterion(mask * generator_output, mask * data_w_noise)\n",
    "\n",
    "    G_loss = G_loss1 + alpha * MSE_train_loss \n",
    "    return G_loss, MSE_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters\n",
    "gain_parameters = {'batch_size': 100,\n",
    "                  'hint_rate': 0.9,\n",
    "                  'alpha': 100,\n",
    "                  'epochs': 10,\n",
    "                  'learning_rate': 0.001,\n",
    "                  'device': 'cpu'}\n",
    "\n",
    "batch_size = gain_parameters['batch_size']\n",
    "hint_rate = gain_parameters['hint_rate']\n",
    "alpha = gain_parameters['alpha']\n",
    "epochs = gain_parameters['epochs']\n",
    "learning_rate = gain_parameters['learning_rate']\n",
    "device = gain_parameters['device']\n",
    "\n",
    "# initialize your generator, discriminator, and optimizers\n",
    "# Note: each optimizer only takes the parameters of one particular model, \n",
    "# since we want each optimizer to optimize only one of the model  \n",
    "gen = Generator(52, 52).to(device)\n",
    "gen.apply(init_weights)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=learning_rate)\n",
    "disc = Discriminator(52, 52).to(device)\n",
    "disc.apply(init_weights)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=learning_rate)\n",
    "\n",
    "# load and scale the data \n",
    "data_x = pd.read_csv('#datasets/Tennessee_Event-Driven/tep_train_extended.csv')\n",
    "data_x = data_x[data_x['simulationRun']==1].drop(columns=['faultNumber','simulationRun','sample']).values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(data_x)\n",
    "data_x_scaled = scaler.transform(data_x)\n",
    "# create missing data and create tensor dataset that includes data and masks\n",
    "dataset = cust_dataloader(data_x_scaled, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss_curr: tensor(26.9355, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99769944., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(997699.4375, grad_fn=<MseLossBackward>)\n",
      "Iter: 0\n",
      "Train_loss: 998.8\n",
      "\n",
      "D_loss_curr: tensor(26.5593, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0058e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1005815.4375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.3866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0379e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1037887.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.8655, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0095e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1009537.3750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0193, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98949416., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(989494.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6932, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0008e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1000758.3750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7279, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0006e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1000574.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.1304, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0296e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1029648.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.9423, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99570888., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(995708.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7078, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0019e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1001857.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6124, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0154e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1015376.2500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.1983, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0116e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1011647.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1329, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0383e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1038261.3750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0650, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98561872., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(985618.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5791, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0151e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1015068.4375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.9419, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0221e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1022095.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1074, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98588048., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(985880.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5858, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0129e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1012875.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6423, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0047e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1004654.3750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.4340, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99526128., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(995261.3125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0431, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(95944920., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(959449.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7167, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99842360., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(998423.6250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1923, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0028e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1002846.7500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7194, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98874456., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(988744.5625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.5434, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(97630576., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(976305.7500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.4423, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0106e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1010646.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0084, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(97588216., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(975882.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1059, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0058e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1005840.2500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0769, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0025e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1002533., grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.8030, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0142e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1014184.2500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1977, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(96981760., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(969817.6250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7010, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0019e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1001901.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(25.6948, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0044e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1004426.6250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5135, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0246e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1024559.4375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.3324, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99159920., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(991599.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.2556, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99930592., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(999305.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5212, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98876352., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(988763.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7546, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0080e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1007963.0625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7900, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0000e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1000006.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5385, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0095e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1009495.4375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.8272, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98968488., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(989684.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.3589, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99259688., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(992596.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7692, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0411e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1041111.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6449, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0082e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1008203., grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss_curr: tensor(26.5106, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99891304., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(998913.0625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.4231, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0047e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1004709.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.4830, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0117e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1011690.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5794, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98376984., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(983769.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(25.9257, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0018e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1001750.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.4325, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0241e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1024055.3125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7007, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98692672., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(986926.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5243, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98327904., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(983279.0625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7056, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0114e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1011416.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.3934, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0126e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1012615.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7429, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0145e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1014501.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1977, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99559328., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(995593.2500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7500, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0249e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1024873.7500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.3795, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(97972760., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(979727.5625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.1784, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0220e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1021952.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5448, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99828784., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(998287.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7692, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99747936., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(997479.3750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.4615, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0193e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1019297.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7692, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0207e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1020673.0625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(25.7885, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0271e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1027079., grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5962, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0159e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1015917.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.9124, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0035e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1003539.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.8119, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0173e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1017293.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.1154, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0241e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1024059.3750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5000, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99528416., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(995284.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7115, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0188e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1018783.6250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.1538, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0056e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1005624.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6538, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99238416., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(992384.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.2692, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99982416., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(999824.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0385, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98034584., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(980345.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5591, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(97974880., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(979748.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.9038, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0123e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1012300.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6584, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99523864., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(995238.6250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.0385, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0211e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1021142.2500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6748, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99872384., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(998723.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7500, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0131e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1013118.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0235e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1023462.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.6346, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98814248., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(988142.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5769, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0005e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1000462., grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6319, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99676384., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(996763.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.8543, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0065e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1006483.6250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5577, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0094e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1009385.8750, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.9231, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99103984., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(991039.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.7885, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(97253720., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(972537.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.2885, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0135e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1013545.2500, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5577, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0023e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1002253., grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.4615, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98444080., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(984440.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.2500, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0119e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1011887.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.4231, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0143e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1014283.9375, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.1346, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0265e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1026494., grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(25.8077, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0072e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1007160.8125, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss_curr: tensor(27.0192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98925352., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(989253.5000, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(27.4423, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98832272., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(988322.6875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5577, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(99363504., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(993635.0625, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.6180, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(97367632., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(973676.3125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.4615, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0326e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1032623.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5017, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0148e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1014847.1875, grad_fn=<MseLossBackward>)\n",
      "Iter: 100\n",
      "Train_loss: 1.007e+03\n",
      "\n",
      "D_loss_curr: tensor(26.5577, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0293e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1029339.1875, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5385, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(96975280., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(969752.8125, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.3202, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(98333112., grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(983331.1250, grad_fn=<MseLossBackward>)\n",
      "D_loss_curr: tensor(26.5577, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "G_loss_curr: tensor(1.0064e+08, grad_fn=<AddBackward0>)\n",
      "MSE_train_loss_curr: tensor(1006418.4375, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs)[:1]:\n",
    "    for i,(data, mask) in enumerate(dataset):\n",
    "        # /100 as noise was added in the original paper from uniform distribution <0,0.01>\n",
    "        noise = (1-mask) * torch.rand(mask.shape)/100\n",
    "        hint_matrix = mask * binary_sampler(hint_rate, mask.shape[0], mask.shape[1])\n",
    "        data_w_noise = data + noise\n",
    "    \n",
    "        disc_opt.zero_grad()\n",
    "        D_loss_curr = discriminator_loss(gen, disc, disc_criterion, mask, data_w_noise, hint_matrix)\n",
    "        print('D_loss_curr: ' + str(D_loss_curr))\n",
    "        D_loss_curr.backward(retain_graph=True)\n",
    "        disc_opt.step()\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        G_loss_curr, MSE_train_loss_curr = generator_loss(gen, disc, gen_criterion, data, mask, data_w_noise, hint_matrix)\n",
    "        print('G_loss_curr: ' + str(G_loss_curr))\n",
    "        print('MSE_train_loss_curr: ' + str(MSE_train_loss_curr))\n",
    "        G_loss_curr.backward(retain_graph=True)\n",
    "        gen_opt.step()\n",
    "\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Iter: {}'.format(i))\n",
    "            print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr.item())))\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
