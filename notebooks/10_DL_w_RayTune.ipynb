{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning and using Raytune and visulization using Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses preprocessed dataset by following [notebook](12_PowerConverter_dataset_preprocessing.ipynb).\n",
    "\n",
    "**notes**\n",
    "* CPU monitoring in terminal:  \n",
    "```bash\n",
    "top\n",
    "```\n",
    "* GPU monitoring in terminal:  \n",
    "```bash\n",
    "pip install gpustat\n",
    "watch -c gpustat -cp --color\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "# to save results to data directory\n",
    "module_path = '..'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from copy import copy, deepcopy\n",
    "import torch\n",
    "import multiprocessing\n",
    "import json\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_widedeep.dataloaders import DataLoaderImbalanced, DataLoaderDefault\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.training import Trainer\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.models.transformers.saint import SAINT\n",
    "from pytorch_widedeep.callbacks import EarlyStopping, ModelCheckpoint, LRHistory, RayTuneReporter\n",
    "from pytorch_widedeep.initializers import KaimingNormal, XavierNormal\n",
    "from pytorch_widedeep.optim import RAdam\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "from torch.optim import Adam, SGD, lr_scheduler\n",
    "from torchmetrics import F1 as F1_torchmetrics\n",
    "from torchmetrics import Accuracy as Accuracy_torchmetrics\n",
    "from torchmetrics import Precision as Precision_torchmetrics\n",
    "from torchmetrics import Recall as Recall_torchmetrics\n",
    "from torchmetrics import MeanSquaredError as MSE_torchmetrics\n",
    "\n",
    "from pytorch_widedeep import Tab2Vec\n",
    "import dill\n",
    "\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "# temporarily remove deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import src\n",
    "from src import common\n",
    "\n",
    "from time import time\n",
    "\n",
    "import re\n",
    "\n",
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "import tracemalloc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = common.json_load(\"#datasets/Colab_PowerConverter/column_types.json\")\n",
    "target = column_types[\"target\"]\n",
    "measurement_label = column_types[\"measurement_label\"]\n",
    "RANDOM_STATE = 1\n",
    "TEST_SIZE_TRAIN = 0.2\n",
    "TEST_SIZE_VALID = 0.5\n",
    "EMBEDDING = False\n",
    "TASK = \"multiclass\" #(or \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"#datasets/Colab_PowerConverter/dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this measurement did not have a fault (?)\n",
    "df = df[df[measurement_label]!=\"Single-Phase_Sensor_Fault\"]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_dict = {}\n",
    "for label,i in zip(df[measurement_label].unique(), range(len(df[measurement_label].unique()))):\n",
    "    df.loc[(df[measurement_label]==label) & (df[target]==1), target] = int(i+1)\n",
    "    fault_dict[label] = int(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     597599\n",
       "5      40014\n",
       "3      40001\n",
       "6      40001\n",
       "7      40001\n",
       "8      40001\n",
       "9      40001\n",
       "10     40001\n",
       "11     40001\n",
       "13     40001\n",
       "1      38971\n",
       "2      38971\n",
       "4       3166\n",
       "12      1335\n",
       "Name: fault, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imbalance of the classes\n",
    "df[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Damping-320': 1,\n",
       " 'Damping-32000': 2,\n",
       " 'Inertia-1.2': 3,\n",
       " 'LL_Fault': 4,\n",
       " 'Three-Phase_Sensor_Fault': 5,\n",
       " 'Weak_Grid-4_5_mH': 6,\n",
       " 'Weak_Grid-1_5_mH': 7,\n",
       " 'Damping-3200': 8,\n",
       " 'Inertia-0.2': 9,\n",
       " 'Inertia-2': 10,\n",
       " 'Single_Phase_Sag': 11,\n",
       " 'Three_Phase_Grid_Fault': 12,\n",
       " 'Weak_Grid-7_5_mH': 13}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fault_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[measurement_label], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=TEST_SIZE_TRAIN, stratify=df[target], random_state=RANDOM_STATE)\n",
    "df_valid, df_test = train_test_split(df_valid, test_size=TEST_SIZE_VALID, stratify=df_valid[target], random_state=RANDOM_STATE)\n",
    "\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_valid.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scaled, Scaler = common.scale(df_train, [target], scaler_sk='Standard')\n",
    "df_valid_scaled, Scaler = common.scale(df_valid, [target], scaler_sk=Scaler)\n",
    "df_test_scaled, Scaler = common.scale(df_test, [target], scaler_sk=Scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = df[target].nunique()\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy_torchmetrics(average=None, num_classes=NUM_CLASSES)\n",
    "precision = Precision_torchmetrics(average='micro', num_classes=NUM_CLASSES)\n",
    "f1 = F1_torchmetrics(average=None, num_classes=NUM_CLASSES)\n",
    "recall = Recall_torchmetrics(average=None, num_classes=NUM_CLASSES)\n",
    "metrics = [accuracy, precision, f1, recall]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer size rules\n",
    "  * [Jeremy Howard](https://www.youtube.com/channel/UCX7Y2qWriXpqocG97SFW2OQ)\n",
    "    * see related discussion [1](https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608) and [2](https://forums.fast.ai/t/embedding-layer-size-rule/50691), but it won't help you :) \n",
    "  * [Google blog rule](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html)\n",
    "* **[Per class metrics](https://pytorch-widedeep.readthedocs.io/en/latest/metrics.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING:\n",
    "    embedding_rule = 'jeremy_old'\n",
    "\n",
    "    embed_cols = []\n",
    "    if embedding_rule == 'google':\n",
    "        embed_input = [(u, round(df_train[u].nunique()**0.25)) for u in embed_cols]\n",
    "    elif embedding_rule == 'jeremy_old':\n",
    "        embed_input = [(u, min(50, df_train[u].nunique()//2)) for u in embed_cols]\n",
    "    elif embedding_rule == 'jeremy_new':\n",
    "        embed_input = [(u, min(600, round(1.6 * df_train[u].nunique()**0.56))) for u in embed_cols]\n",
    "else:\n",
    "    embed_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = df.drop(columns=[target]).columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deeptabular\n",
    "tab_preprocessor = TabPreprocessor(embed_cols=embed_input,\n",
    "                                   continuous_cols=cont_cols,\n",
    "                                   shared_embed=False,\n",
    "                                   scale=False)\n",
    "X_tab_train = tab_preprocessor.fit_transform(df_train_scaled)\n",
    "X_tab_valid = tab_preprocessor.transform(df_valid_scaled)\n",
    "X_tab_test = tab_preprocessor.transform(df_test_scaled)\n",
    "\n",
    "# target\n",
    "y_train = df_train_scaled[target].values\n",
    "y_valid = df_valid_scaled[target].values\n",
    "y_test = df_test_scaled[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = len(tab_preprocessor.continuous_cols)\n",
    "if TASK == \"multiclass\":\n",
    "    output_layer = NUM_CLASSES\n",
    "if TASK == \"binary\":\n",
    "    output_layer = 1\n",
    "hidden_layers = np.linspace(input_layer*2, output_layer, 5, endpoint=False, dtype=int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (deeptabular): Sequential(\n",
       "    (0): TabMlp(\n",
       "      (cat_embed_and_cont): CatEmbeddingsAndCont(\n",
       "        (cont_norm): BatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (tab_mlp): MLP(\n",
       "        (mlp): Sequential(\n",
       "          (dense_layer_0): Sequential(\n",
       "            (0): Linear(in_features=13, out_features=26, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dense_layer_1): Sequential(\n",
       "            (0): Linear(in_features=26, out_features=23, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dense_layer_2): Sequential(\n",
       "            (0): Linear(in_features=23, out_features=21, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dense_layer_3): Sequential(\n",
       "            (0): Linear(in_features=21, out_features=18, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dense_layer_4): Sequential(\n",
       "            (0): Linear(in_features=18, out_features=16, bias=False)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=16, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if EMBEDDING:\n",
    "    deeptabular = TabMlp(mlp_hidden_dims=hidden_layers,\n",
    "                         column_idx=tab_preprocessor.column_idx,\n",
    "                         embed_input=tab_preprocessor.embeddings_input,\n",
    "                         continuous_cols=tab_preprocessor.continuous_cols,\n",
    "                         mlp_batchnorm=True,\n",
    "                         mlp_batchnorm_last=True,\n",
    "                         mlp_linear_first=True)\n",
    "else:\n",
    "    deeptabular = TabMlp(mlp_hidden_dims=hidden_layers,\n",
    "                     column_idx=tab_preprocessor.column_idx,\n",
    "                     continuous_cols=tab_preprocessor.continuous_cols,\n",
    "                     mlp_batchnorm=True,\n",
    "                     mlp_batchnorm_last=True,\n",
    "                     mlp_linear_first=True)\n",
    "\n",
    "model = WideDeep(deeptabular=deeptabular, pred_dim=output_layer)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "deep_opt = SGD(model.deeptabular.parameters(), lr=0.1)\n",
    "# LR Schedulers\n",
    "deep_sch = lr_scheduler.StepLR(deep_opt, step_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|█| 748/748 [00:09<00:00, 81.72it/s, loss=0.105, metrics={'Accuracy': [0.075, 0.4447, 0.0113\n",
      "valid: 100%|█| 1041/1041 [00:10<00:00, 96.47it/s, loss=0.0466, metrics={'Accuracy': [0.0034, 0.8101, 0.0,\n",
      "epoch 2: 100%|█| 748/748 [00:10<00:00, 74.68it/s, loss=0.0376, metrics={'Accuracy': [0.0548, 0.6595, 0.00\n",
      "valid: 100%|█| 1041/1041 [00:11<00:00, 93.48it/s, loss=0.0286, metrics={'Accuracy': [0.0008, 0.9956, 0.0,\n",
      "epoch 3: 100%|█| 748/748 [00:13<00:00, 55.55it/s, loss=0.03, metrics={'Accuracy': [0.0773, 0.6895, 0.0086\n",
      "valid: 100%|█| 1041/1041 [00:11<00:00, 87.92it/s, loss=0.0285, metrics={'Accuracy': [0.0019, 0.9941, 0.0,\n",
      "epoch 4: 100%|█| 748/748 [00:10<00:00, 70.84it/s, loss=0.0294, metrics={'Accuracy': [0.0822, 0.6876, 0.01\n",
      "valid: 100%|█| 1041/1041 [00:11<00:00, 89.74it/s, loss=0.0274, metrics={'Accuracy': [0.0143, 1.0, 0.0, 0.\n",
      "epoch 5: 100%|█| 748/748 [00:10<00:00, 68.36it/s, loss=0.029, metrics={'Accuracy': [0.0832, 0.7022, 0.009\n",
      "valid: 100%|█| 1041/1041 [00:12<00:00, 84.57it/s, loss=0.0274, metrics={'Accuracy': [0.0208, 1.0, 0.0, 0.\n",
      "epoch 6: 100%|█| 748/748 [00:12<00:00, 61.49it/s, loss=0.029, metrics={'Accuracy': [0.0832, 0.6918, 0.011\n",
      "valid: 100%|█| 1041/1041 [00:13<00:00, 75.65it/s, loss=0.027, metrics={'Accuracy': [0.0136, 0.9489, 0.0, \n",
      "epoch 7: 100%|█| 748/748 [00:11<00:00, 67.39it/s, loss=0.0289, metrics={'Accuracy': [0.079, 0.6971, 0.008\n",
      "valid: 100%|█| 1041/1041 [00:11<00:00, 87.01it/s, loss=0.0275, metrics={'Accuracy': [0.0095, 0.9995, 0.0,\n",
      "epoch 8: 100%|█| 748/748 [00:15<00:00, 47.47it/s, loss=0.029, metrics={'Accuracy': [0.0735, 0.6869, 0.013\n",
      "valid: 100%|█| 1041/1041 [00:14<00:00, 73.14it/s, loss=0.0274, metrics={'Accuracy': [0.009, 1.0, 0.0, 0.0\n",
      "epoch 9: 100%|█| 748/748 [00:13<00:00, 53.98it/s, loss=0.0289, metrics={'Accuracy': [0.0777, 0.6961, 0.01\n",
      "valid: 100%|█| 1041/1041 [00:12<00:00, 81.94it/s, loss=0.0271, metrics={'Accuracy': [0.0111, 0.9895, 0.0,\n",
      "epoch 10: 100%|█| 748/748 [00:11<00:00, 66.13it/s, loss=0.0289, metrics={'Accuracy': [0.0786, 0.6905, 0.0\n",
      "valid: 100%|█| 1041/1041 [00:13<00:00, 77.42it/s, loss=0.0275, metrics={'Accuracy': [0.0114, 1.0, 0.0, 0.\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping()\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=\"#temp_models/\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    "    max_save=1,\n",
    ")\n",
    "\n",
    "if TASK == \"binary\":\n",
    "    objective = \"binary_focal_loss\"\n",
    "    dataloader = DataLoaderImbalanced\n",
    "if TASK == \"multiclass\":\n",
    "    objective = \"multiclass_focal_loss\"\n",
    "    dataloader = DataLoaderImbalanced   \n",
    "    \n",
    "trainer = Trainer(model,\n",
    "                  objective=objective,\n",
    "                  callbacks=[LRHistory(n_epochs=10)],\n",
    "                  lr_schedulers={'deeptabular': deep_sch},\n",
    "                  initializers={'deeptabular': XavierNormal},\n",
    "                  optimizers={'deeptabular': deep_opt},\n",
    "                  metrics=metrics)\n",
    "\n",
    "trainer.fit(X_train={\"X_tab\": X_tab_train, \"target\": y_train},\n",
    "            X_val={\"X_tab\": X_tab_valid, \"target\": y_valid},\n",
    "            n_epochs=10,\n",
    "            batch_size=100,\n",
    "            custom_dataloader=dataloader,\n",
    "            oversample_mul=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████████████████████████████████████████████████| 1041/1041 [00:04<00:00, 253.97it/s]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame({\"predicted\": trainer.predict(X_tab=X_tab_test),\n",
    "                       \"ground_truth\": df_test[target].values,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.95      0.02       747\n",
      "           1       1.00      0.28      0.43     14139\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.07      0.02      0.03     16889\n",
      "           4       0.16      0.01      0.02      6015\n",
      "           5       1.00      0.86      0.92      4652\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.09      0.08      0.08      4382\n",
      "           8       0.05      0.03      0.04      8198\n",
      "           9       0.15      0.10      0.12      6105\n",
      "          10       0.02      0.12      0.04       739\n",
      "          11       0.23      0.05      0.08     18332\n",
      "          12       0.92      0.42      0.58       291\n",
      "          13       0.41      0.07      0.12     23518\n",
      "\n",
      "    accuracy                           0.12    104007\n",
      "   macro avg       0.29      0.21      0.18    104007\n",
      "weighted avg       0.35      0.12      0.16    104007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:\\n{}'.format(classification_report(result['predicted'], result['ground_truth'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo prediction (uncertainty)\n",
    "* requires install of pytorch-widedeep branch that was not yet merged to master - https://github.com/jrzaurin/pytorch-widedeep/tree/pmulinka/uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict_UncertaintyIter: 100%|█████████████████████████████████████████████| 5/5 [00:15<00:00,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "df_pred_unc = trainer.predict_uncertainty(X_tab=X_tab_test, uncertainty_granularity=5)\n",
    "result = pd.DataFrame({\"predicted\": df_pred_unc[:,-1],\n",
    "                       \"ground_truth\": df_test[target].values,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.04      0.89      0.08      2847\n",
      "         1.0       0.98      0.22      0.36     17144\n",
      "         2.0       0.00      0.02      0.00       413\n",
      "         3.0       0.06      0.02      0.03     12746\n",
      "         4.0       0.09      0.01      0.01      4305\n",
      "         5.0       0.94      0.77      0.85      4877\n",
      "         6.0       0.01      0.08      0.01       309\n",
      "         7.0       0.04      0.06      0.05      2803\n",
      "         8.0       0.10      0.04      0.06      9666\n",
      "         9.0       0.13      0.07      0.09      7258\n",
      "        10.0       0.06      0.09      0.07      2415\n",
      "        11.0       0.31      0.06      0.09     22332\n",
      "        12.0       0.95      0.13      0.22       993\n",
      "        13.0       0.25      0.06      0.10     15899\n",
      "\n",
      "    accuracy                           0.14    104007\n",
      "   macro avg       0.28      0.18      0.15    104007\n",
      "weighted avg       0.35      0.14      0.16    104007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:\\n{}'.format(classification_report(result['predicted'], result['ground_truth'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the dictionary from a learned model\n",
    "* only in case we are using embeddings that we want to use in other models\n",
    "* Use built-in Tab2Vec with model and tab_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor, return_dataframe=True)\n",
    "# df_test_scaled_enc, df_test_y = t2v.transform(df_test_scaled, target_col=target_col)\n",
    "# df_test_scaled_eanc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dill \"lesson learned\" :\n",
    "* issue\n",
    "```\n",
    "you fit scalers, transformers, preprocessors, label encoders, models, etc. and you need to save them so you could use them when necessary for predictions, ie. in prediction script that is used everyday\n",
    "```\n",
    "* wrong solution\n",
    "```\n",
    "use pickle to store the objects in files, or even better - create named dictionary with all models that includes the objects\n",
    "```\n",
    "* why is it wrong?\n",
    "```\n",
    "As it is nicely described in this [post](https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence/25119089#25119089) , pickle serializes the objects but uses references to the objects, ie. if you change anything in your code, or if you had \"ad-hoc\" class in notebook that you then move to repo/library then the object will be empty/non-initialized\n",
    "better approach.\n",
    "As mentioned in the post use \"dill\" that serializes also the class definition, you can also save the whole session, e.g. when you finished working in a jupyter notebook for the day and yo udo not want run all cells after you respawn the machine next day\n",
    "```\n",
    "* wanna get fancy?\n",
    "  * you can use also klepto which extends dill with \"nifty\" archive types, e.g.:\n",
    "    * file_archive - a dictionary-style interface to a file\n",
    "    * dir_archive - a dictionary-style interface to a folder of files\n",
    "  * https://klepto.readthedocs.io/en/latest/index.html\n",
    "  * https://pypi.org/project/dill/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"#temp_models/dl_entity_emb_model.dill\", \"wb\") as f:\n",
    "#     dill.dump(model, f)\n",
    "# with open(\"#temp_models/dl_entity_emb_model_tab_preprocessor.dill\", \"wb\") as f:\n",
    "#     dill.dump(tab_preprocessor, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w Raytune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-05 00:27:15 (running for 00:01:54.21)<br>Memory usage on this node: 2.4/12.2 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.7 GiB heap, 0.0/3.35 GiB objects<br>Result logdir: /home/palo/ray_results/training_function_2021-11-05_00-25-21<br>Number of trials: 8/8 (8 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  batch_size</th><th>deep_opt  </th><th>deep_sch                                                  </th><th>hidden_layers       </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_7afc0_00000</td><td>TERMINATED</td><td>172.27.173.201:8451</td><td style=\"text-align: right;\">        1000</td><td>SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.1\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f904d3c70&gt;</td><td>[26, 22, 18]        </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         73.5314</td></tr>\n",
       "<tr><td>training_function_7afc0_00001</td><td>TERMINATED</td><td>172.27.173.201:8454</td><td style=\"text-align: right;\">       10000</td><td>SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.1\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f9047ea90&gt;</td><td>[26, 22, 18]        </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         52.1587</td></tr>\n",
       "<tr><td>training_function_7afc0_00002</td><td>TERMINATED</td><td>172.27.173.201:8448</td><td style=\"text-align: right;\">        1000</td><td>Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.1\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f91157fa0&gt;</td><td>[26, 22, 18]        </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         65.7284</td></tr>\n",
       "<tr><td>training_function_7afc0_00003</td><td>TERMINATED</td><td>172.27.173.201:8449</td><td style=\"text-align: right;\">       10000</td><td>Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.1\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f9049d5b0&gt;</td><td>[26, 22, 18]        </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         52.5479</td></tr>\n",
       "<tr><td>training_function_7afc0_00004</td><td>TERMINATED</td><td>172.27.173.201:8452</td><td style=\"text-align: right;\">        1000</td><td>SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.1\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f9050a8b0&gt;</td><td>[26, 23, 21, 18, 16]</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         70.8522</td></tr>\n",
       "<tr><td>training_function_7afc0_00005</td><td>TERMINATED</td><td>172.27.173.201:8450</td><td style=\"text-align: right;\">       10000</td><td>SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.1\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f9050aeb0&gt;</td><td>[26, 23, 21, 18, 16]</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         57.7705</td></tr>\n",
       "<tr><td>training_function_7afc0_00006</td><td>TERMINATED</td><td>172.27.173.201:8447</td><td style=\"text-align: right;\">        1000</td><td>Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.1\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f9050af70&gt;</td><td>[26, 23, 21, 18, 16]</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         69.5162</td></tr>\n",
       "<tr><td>training_function_7afc0_00007</td><td>TERMINATED</td><td>172.27.173.201:8965</td><td style=\"text-align: right;\">       10000</td><td>Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.1\n",
       "    weight_decay: 0\n",
       ")           </td><td>&lt;torch.optim.lr_scheduler.StepLR object at 0x7f7f90495580&gt;</td><td>[26, 23, 21, 18, 16]</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         49.4401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 00:27:16,111\tINFO tune.py:630 -- Total run time: 114.35 seconds (113.77 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Optimizers\n",
    "deep_opt_sgd_01 = SGD(model.deeptabular.parameters(), lr=0.1)\n",
    "deep_opt_adam_01 = Adam(model.deeptabular.parameters(), lr=0.1)\n",
    "# LR Schedulers\n",
    "deep_sch_StepLR5 = lr_scheduler.StepLR(deep_opt, step_size=5)\n",
    "\n",
    "input_layer = len(tab_preprocessor.continuous_cols)\n",
    "if TASK == \"multiclass\":\n",
    "    output_layer = NUM_CLASSES\n",
    "if TASK == \"binary\":\n",
    "    output_layer = 1\n",
    "hidden_layers3 = np.linspace(\n",
    "    input_layer * 2, output_layer, 3, endpoint=False, dtype=int\n",
    ").tolist()\n",
    "hidden_layers5 = np.linspace(\n",
    "    input_layer * 2, output_layer, 5, endpoint=False, dtype=int\n",
    ").tolist()\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": tune.grid_search([1000, 10000]),\n",
    "    \"deep_opt\": tune.grid_search([deep_opt_sgd_01, deep_opt_adam_01]),\n",
    "    \"deep_sch\": tune.grid_search([deep_sch_StepLR5]),\n",
    "    \"hidden_layers\": tune.grid_search([hidden_layers3, hidden_layers5]),\n",
    "    \"wandb\": {\n",
    "        \"project\": \"test\",\n",
    "        \"api_key_file\": \"src/wandb_api.key\",\n",
    "        \"log_config\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "if TASK == \"binary\":\n",
    "    objective = \"binary_focal_loss\"\n",
    "    dataloader = DataLoaderImbalanced\n",
    "if TASK == \"multiclass\":\n",
    "    objective = \"multiclass_focal_loss\"\n",
    "    dataloader = DataLoaderImbalanced\n",
    "\n",
    "\n",
    "def training_function(config, X_train, X_val):\n",
    "    deeptabular = TabMlp(\n",
    "        mlp_hidden_dims=config[\"hidden_layers\"],\n",
    "        column_idx=tab_preprocessor.column_idx,\n",
    "        continuous_cols=tab_preprocessor.continuous_cols,\n",
    "        mlp_batchnorm=True,\n",
    "        mlp_batchnorm_last=True,\n",
    "        mlp_linear_first=True,\n",
    "    )\n",
    "\n",
    "    model = WideDeep(deeptabular=deeptabular, pred_dim=output_layer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        objective=objective,\n",
    "        callbacks=[RayTuneReporter, LRHistory(n_epochs=10)],\n",
    "        lr_schedulers={\"deeptabular\": config[\"deep_sch\"]},\n",
    "        initializers={\"deeptabular\": XavierNormal},\n",
    "        optimizers={\"deeptabular\": config[\"deep_opt\"]},\n",
    "        metrics=metrics,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        X_train=X_train,\n",
    "        X_val=X_val,\n",
    "        n_epochs=5,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        custom_dataloader=dataloader,\n",
    "        oversample_mul=5,\n",
    "    )\n",
    "\n",
    "\n",
    "X_train = {\"X_tab\": X_tab_train, \"target\": y_train}\n",
    "X_val = {\"X_tab\": X_tab_valid, \"target\": y_valid}\n",
    "\n",
    "# https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#tune-scheduler-hyperband\n",
    "asha_scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"_metric/val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(training_function, X_train=X_train, X_val=X_val),\n",
    "    # resources_per_trial={\"cpu\": 4, \"gpu\": 0},\n",
    "    num_samples=1,\n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=True),\n",
    "    scheduler=asha_scheduler,\n",
    "    config=config,\n",
    "    loggers=DEFAULT_LOGGERS + (WandbLogger,),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**best params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1000,\n",
       " 'deep_opt': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.1\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'deep_sch': <torch.optim.lr_scheduler.StepLR at 0x7f7f9050af70>,\n",
       " 'hidden_layers': [26, 23, 21, 18, 16]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = copy(analysis.get_best_config(\"_metric/val_loss\", \"min\"))\n",
    "params.pop(\"wandb\")\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e69042d52b34ea98\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e69042d52b34ea98\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ~/ray_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression for comparisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_model = LogisticRegression(random_state=RANDOM_STATE).fit(df_train_scaled.drop(columns=[target]), df_train_scaled[target])\n",
    "result = pd.DataFrame({\"predicted\": LogReg_model.predict(df_test_scaled.drop(columns=[target])),\n",
    "                       \"ground_truth\": df_test[target].values,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.84      0.90     68348\n",
      "           1       0.42      0.59      0.49      2744\n",
      "           2       0.30      0.10      0.15     11340\n",
      "           3       0.14      0.11      0.12      5134\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.22      0.32      0.26      2764\n",
      "           6       0.00      0.00      0.00       163\n",
      "           7       0.16      0.14      0.15      4564\n",
      "           8       0.01      0.08      0.02       659\n",
      "           9       0.13      0.12      0.12      4458\n",
      "          10       0.03      0.09      0.05      1491\n",
      "          11       0.12      0.28      0.16      1643\n",
      "          12       0.92      0.91      0.92       135\n",
      "          13       0.00      0.00      0.00       564\n",
      "\n",
      "    accuracy                           0.61    104007\n",
      "   macro avg       0.24      0.26      0.24    104007\n",
      "weighted avg       0.71      0.61      0.65    104007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:\\n{}'.format(classification_report(result['predicted'], result['ground_truth'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different hidden layer designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = len(tab_preprocessor.continuous_cols)\n",
    "output_layer = 1\n",
    "\n",
    "lengths = [3,5,7,9,11]\n",
    "\n",
    "pipes = []\n",
    "anti_autoencoders = []\n",
    "trapezoids = []\n",
    "anti_trapezoids = []\n",
    "funnels = []\n",
    "adj_funnels = []\n",
    "apollos = []\n",
    "\n",
    "for length in lengths:\n",
    "    pipe = [input_layer]*length\n",
    "    pipes.append(pipe)\n",
    "    anti_autoencoder = np.linspace(input_layer, input_layer*2, ceil(length/2), dtype=int).tolist()\n",
    "    anti_autoencoder.extend(anti_autoencoder[-2::-1])\n",
    "    anti_autoencoders.append(anti_autoencoder)\n",
    "    trapezoid = np.array([round(input_layer*1.25)]*length)\n",
    "    trapezoid[[0, -1]] = input_layer\n",
    "    trapezoids.append(trapezoid.tolist())\n",
    "    anti_trapezoid = np.array([round(input_layer*0.75)]*length)\n",
    "    anti_trapezoid[[0, -1]] = input_layer\n",
    "    anti_trapezoids.append(anti_trapezoid.tolist())\n",
    "    funnel = np.linspace(input_layer*2, output_layer, length, endpoint=False, dtype=int).tolist()\n",
    "    funnels.append(funnel)\n",
    "    adj_funnel = np.linspace(input_layer*2, output_layer, length, endpoint=False, dtype=int).tolist()\n",
    "    adj_funnel.insert(0, input_layer)\n",
    "    adj_funnels.append(adj_funnel)\n",
    "    apollo = np.linspace(input_layer, input_layer*2, length, dtype=int).tolist()\n",
    "    apollos.append(apollo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
