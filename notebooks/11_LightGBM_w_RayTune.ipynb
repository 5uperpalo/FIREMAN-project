{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This notebook uses preprocessed dataset by following [notebook](12_PowerConverter_dataset_preprocessing.ipynb).\n",
    "\n",
    "**notes**\n",
    "* CPU monitoring in terminal:  \n",
    "```bash\n",
    "top\n",
    "```\n",
    "* GPU monitoring in terminal:  \n",
    "```bash\n",
    "pip install gpustat\n",
    "watch -c gpustat -cp --color\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# to save results to data directory\n",
    "module_path = \"..\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(1, module_path)\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tracemalloc\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from typing import Union\n",
    "\n",
    "import dill\n",
    "import lightgbm as lgbm\n",
    "import lime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from eli5 import explain_prediction_df, explain_weights, explain_weights_df\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from pytorch_widedeep import Tab2Vec\n",
    "from pytorch_widedeep.utils import LabelEncoder\n",
    "from sklearn.metrics import classification_report, log_loss, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import src\n",
    "from src import common\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "import tracemalloc\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.integration.lightgbm import TuneReportCheckpointCallback\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "# temporarily remove deprecation warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_types = common.json_load(\"#datasets/Colab_PowerConverter/column_types.json\")\n",
    "target = column_types[\"target\"]\n",
    "measurement_label = column_types[\"measurement_label\"]\n",
    "RANDOM_STATE = 1\n",
    "TEST_SIZE_TRAIN = 0.2\n",
    "TEST_SIZE_VALID = 0.5\n",
    "EMBEDDING = False\n",
    "TASK = \"multiclass\"  # (or \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"#datasets/Colab_PowerConverter/dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this measurement did not have a fault (?)\n",
    "df = df[df[measurement_label] != \"Single-Phase_Sensor_Fault\"]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_dict = {}\n",
    "for label, i in zip(df[measurement_label].unique(), range(len(df[measurement_label].unique()))):\n",
    "    df.loc[(df[measurement_label] == label) & (df[target] == 1), target] = int(i + 1)\n",
    "    fault_dict[label] = int(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     597599\n",
       "5      40014\n",
       "3      40001\n",
       "6      40001\n",
       "7      40001\n",
       "8      40001\n",
       "9      40001\n",
       "10     40001\n",
       "11     40001\n",
       "13     40001\n",
       "1      38971\n",
       "2      38971\n",
       "4       3166\n",
       "12      1335\n",
       "Name: fault, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imbalance of the classes\n",
    "df[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Damping-320': 1,\n",
       " 'Damping-32000': 2,\n",
       " 'Inertia-1.2': 3,\n",
       " 'LL_Fault': 4,\n",
       " 'Three-Phase_Sensor_Fault': 5,\n",
       " 'Weak_Grid-4_5_mH': 6,\n",
       " 'Weak_Grid-1_5_mH': 7,\n",
       " 'Damping-3200': 8,\n",
       " 'Inertia-0.2': 9,\n",
       " 'Inertia-2': 10,\n",
       " 'Single_Phase_Sag': 11,\n",
       " 'Three_Phase_Grid_Fault': 12,\n",
       " 'Weak_Grid-7_5_mH': 13}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fault_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[measurement_label], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=TEST_SIZE_TRAIN, stratify=df[target], random_state=RANDOM_STATE)\n",
    "df_valid, df_test = train_test_split(\n",
    "    df_valid,\n",
    "    test_size=TEST_SIZE_VALID,\n",
    "    stratify=df_valid[target],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_valid.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scaled, Scaler = common.scale(df_train, [target], scaler_sk=\"Standard\")\n",
    "df_valid_scaled, Scaler = common.scale(df_valid, [target], scaler_sk=Scaler)\n",
    "df_test_scaled, Scaler = common.scale(df_test, [target], scaler_sk=Scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMBEDDING:\n",
    "    CAT_FEATURE_TRANSFORMATION = \"Entity Embedding\"\n",
    "    if CAT_FEATURE_TRANSFORMATION == \"Label Encoding\":\n",
    "        label_encoder = LabelEncoder(cat_cols)\n",
    "        label_encoder.fit(data[cat_cols])\n",
    "\n",
    "        df_train_scaled_enc = df_train_scaled.copy()\n",
    "        df_valid_scaled_enc = df_valid_scaled.copy()\n",
    "        df_test_scaled_enc = df_test_scaled.copy()\n",
    "\n",
    "        df_train_scaled_enc[cat_cols] = label_encoder.transform(df_train_scaled_enc[cat_cols])\n",
    "        df_valid_scaled_enc[cat_cols] = label_encoder.transform(df_valid_scaled_enc[cat_cols])\n",
    "        df_test_scaled_enc[cat_cols] = label_encoder.transform(df_test_scaled_enc[cat_cols])\n",
    "        df_test_scaled_enc[cat_cols].head()\n",
    "\n",
    "    if CAT_FEATURE_TRANSFORMATION == \"Entity Embedding\":\n",
    "        # using pretrained embedding from pytorch-widedeep model and its tab_preprocessor\n",
    "        with open(\"dl_entity_emb_model_\" + TASK + \".dill\", \"rb\") as f:\n",
    "            model = dill.load(f)\n",
    "        with open(\"dl_entity_emb_model_tab_preprocessor_\" + TASK + \".dill\", \"rb\") as f:\n",
    "            tab_preprocessor = dill.load(f)\n",
    "\n",
    "        t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor, return_dataframe=True)\n",
    "        df_train_scaled_enc, df_train_y = t2v.transform(df_train_scaled, target_col=target_col)\n",
    "        df_valid_scaled_enc, df_valid_y = t2v.transform(df_valid_scaled, target_col=target_col)\n",
    "        df_test_scaled_enc, df_test_y = t2v.transform(df_test_scaled, target_col=target_col)\n",
    "        df_train_scaled_enc[target_col] = df_train_y\n",
    "        df_valid_scaled_enc[target_col] = df_valid_y\n",
    "        df_test_scaled_enc[target_col] = df_test_y\n",
    "\n",
    "        cols_list = list(df_test_scaled_enc.columns)\n",
    "        cat_cols_emb = []\n",
    "        for cat_col in cat_cols:\n",
    "            r = re.compile(cat_col + \"*\")\n",
    "            cat_cols_emb.extend(list(filter(r.match, cols_list)))\n",
    "    # df_test_scaled_enc[cat_cols_emb].head()\n",
    "else:\n",
    "    df_train_scaled_enc = df_train_scaled.copy()\n",
    "    df_valid_scaled_enc = df_valid_scaled.copy()\n",
    "    df_test_scaled_enc = df_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_scaled_enc = df_train_scaled_enc.sample(100000)\n",
    "# df_valid_scaled_enc = df_valid_scaled_enc.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = df[target].nunique()\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset, metric and objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "if TASK == \"binary\" or TASK == \"multiclass\":\n",
    "    config[\"objective\"] = TASK\n",
    "    config[\"num_classes\"] = NUM_CLASSES\n",
    "\n",
    "if TASK == \"multiclass\":\n",
    "    ray_metric = \"multi_logloss\"\n",
    "\n",
    "if EMBEDDING:\n",
    "    if CAT_FEATURE_TRANSFORMATION == \"Label Encoding\":\n",
    "        lgb_cat_cols = cat_cols_f\n",
    "    if CAT_FEATURE_TRANSFORMATION == \"Entity Embedding\":\n",
    "        lgb_cat_cols = []\n",
    "else:\n",
    "    lgb_cat_cols = []\n",
    "\n",
    "lgbtrain = lgbm.Dataset(\n",
    "    df_train_scaled_enc.drop(columns=[target]),\n",
    "    df_train_scaled_enc[target],\n",
    "    categorical_feature=lgb_cat_cols,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "lgbvalid = lgbm.Dataset(\n",
    "    df_valid_scaled_enc.drop(columns=[target]),\n",
    "    df_valid_scaled_enc[target],\n",
    "    reference=lgbtrain,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "# Final TRAIN/TEST\n",
    "ftrain = pd.concat([df_train_scaled_enc, df_valid_scaled_enc]).reset_index(drop=True)\n",
    "flgbtrain = lgbm.Dataset(\n",
    "    ftrain.drop(columns=[target]),\n",
    "    ftrain[target],\n",
    "    categorical_feature=lgb_cat_cols,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "lgbtest = lgbm.Dataset(\n",
    "    df_test_scaled_enc.drop(columns=[target]),\n",
    "    df_test_scaled_enc[target],\n",
    "    categorical_feature=lgb_cat_cols,\n",
    "    reference=flgbtrain,\n",
    "    free_raw_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/palo/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "/home/palo/miniconda3/lib/python3.8/site-packages/lightgbm/basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2295\n",
      "[LightGBM] [Info] Number of data points in the train set: 936057, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score -0.554117\n",
      "[LightGBM] [Info] Start training from score -3.284216\n",
      "[LightGBM] [Info] Start training from score -3.284216\n",
      "[LightGBM] [Info] Start training from score -3.258130\n",
      "[LightGBM] [Info] Start training from score -5.794357\n",
      "[LightGBM] [Info] Start training from score -3.257824\n",
      "[LightGBM] [Info] Start training from score -3.258130\n",
      "[LightGBM] [Info] Start training from score -3.258130\n",
      "[LightGBM] [Info] Start training from score -3.258157\n",
      "[LightGBM] [Info] Start training from score -3.258157\n",
      "[LightGBM] [Info] Start training from score -3.258130\n",
      "[LightGBM] [Info] Start training from score -3.258130\n",
      "[LightGBM] [Info] Start training from score -6.657690\n",
      "[LightGBM] [Info] Start training from score -3.258130\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\t's multi_logloss: 0.977783\n",
      "[2]\t's multi_logloss: 0.815664\n",
      "[3]\t's multi_logloss: 0.698939\n",
      "[4]\t's multi_logloss: 0.625425\n",
      "[5]\t's multi_logloss: 0.543057\n",
      "[6]\t's multi_logloss: 0.484832\n",
      "[7]\t's multi_logloss: 0.446497\n",
      "[8]\t's multi_logloss: 0.39927\n",
      "[9]\t's multi_logloss: 0.367568\n",
      "[10]\t's multi_logloss: 0.340573\n",
      "[11]\t's multi_logloss: 0.319405\n",
      "[12]\t's multi_logloss: 0.296019\n",
      "[13]\t's multi_logloss: 0.279713\n",
      "[14]\t's multi_logloss: 0.266099\n",
      "[15]\t's multi_logloss: 0.267354\n",
      "[16]\t's multi_logloss: 0.255597\n",
      "[17]\t's multi_logloss: 0.252282\n",
      "[18]\t's multi_logloss: 0.235086\n",
      "[19]\t's multi_logloss: 0.237021\n",
      "[20]\t's multi_logloss: 0.230893\n",
      "[21]\t's multi_logloss: 0.235814\n",
      "[22]\t's multi_logloss: 0.229844\n",
      "[23]\t's multi_logloss: 0.253783\n",
      "[24]\t's multi_logloss: 0.243563\n",
      "[25]\t's multi_logloss: 0.245112\n",
      "[26]\t's multi_logloss: 0.251267\n",
      "[27]\t's multi_logloss: 0.237238\n",
      "[28]\t's multi_logloss: 0.252955\n",
      "[29]\t's multi_logloss: 0.233645\n",
      "[30]\t's multi_logloss: 0.25831\n",
      "[31]\t's multi_logloss: 0.298165\n",
      "[32]\t's multi_logloss: 0.295286\n",
      "[33]\t's multi_logloss: 0.27396\n",
      "[34]\t's multi_logloss: 0.260525\n",
      "[35]\t's multi_logloss: 0.292285\n",
      "[36]\t's multi_logloss: 0.376081\n",
      "[37]\t's multi_logloss: 0.330623\n",
      "[38]\t's multi_logloss: 0.442323\n",
      "[39]\t's multi_logloss: 0.430259\n",
      "[40]\t's multi_logloss: 0.390212\n",
      "[41]\t's multi_logloss: 0.466535\n",
      "[42]\t's multi_logloss: 0.564834\n",
      "[43]\t's multi_logloss: 0.696845\n",
      "[44]\t's multi_logloss: 0.49243\n",
      "[45]\t's multi_logloss: 0.600275\n",
      "[46]\t's multi_logloss: 0.598877\n",
      "[47]\t's multi_logloss: 0.446685\n",
      "[48]\t's multi_logloss: 0.490207\n",
      "[49]\t's multi_logloss: 0.413613\n",
      "[50]\t's multi_logloss: 0.4442\n",
      "[51]\t's multi_logloss: 0.523439\n",
      "[52]\t's multi_logloss: 0.519875\n",
      "[53]\t's multi_logloss: 0.632309\n",
      "[54]\t's multi_logloss: 0.672342\n",
      "[55]\t's multi_logloss: 0.602289\n",
      "[56]\t's multi_logloss: 0.564768\n",
      "[57]\t's multi_logloss: 0.529735\n",
      "[58]\t's multi_logloss: 0.689203\n",
      "[59]\t's multi_logloss: 2.90619\n",
      "[60]\t's multi_logloss: 0.622281\n",
      "[61]\t's multi_logloss: 0.841785\n",
      "[62]\t's multi_logloss: 0.72034\n",
      "[63]\t's multi_logloss: 0.759707\n",
      "[64]\t's multi_logloss: 0.840372\n",
      "[65]\t's multi_logloss: 0.748165\n",
      "[66]\t's multi_logloss: 0.82118\n",
      "[67]\t's multi_logloss: 1.46753\n",
      "[68]\t's multi_logloss: 1.55493\n",
      "[69]\t's multi_logloss: 1.73801\n",
      "[70]\t's multi_logloss: 1.40299\n",
      "[71]\t's multi_logloss: 3.806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\t's multi_logloss: 2.32776\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\t's multi_logloss: 2.76761\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\t's multi_logloss: 1.50927\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\t's multi_logloss: 1.56427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\t's multi_logloss: 1.5167\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\t's multi_logloss: 1.4302\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\t's multi_logloss: 1.52527\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\t's multi_logloss: 1.54124\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\t's multi_logloss: 1.48194\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\t's multi_logloss: 1.98842\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\t's multi_logloss: 2.02313\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\t's multi_logloss: 3.80615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\t's multi_logloss: 2.92285\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\t's multi_logloss: 2.73017\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\t's multi_logloss: 2.81455\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\t's multi_logloss: 2.67801\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[88]\t's multi_logloss: 3.59612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[89]\t's multi_logloss: 2.67073\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[90]\t's multi_logloss: 2.70365\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[91]\t's multi_logloss: 2.6407\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[92]\t's multi_logloss: 2.73322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[93]\t's multi_logloss: 2.74708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\t's multi_logloss: 3.23696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[95]\t's multi_logloss: 3.33012\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\t's multi_logloss: 3.47458\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\t's multi_logloss: 3.88501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[98]\t's multi_logloss: 4.22404\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\t's multi_logloss: 4.21596\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\t's multi_logloss: 4.65451\n",
      "CPU times: user 5min 27s, sys: 3.14 s, total: 5min 30s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = lgbm.train(\n",
    "    config,\n",
    "    flgbtrain,\n",
    "    valid_sets=[lgbvalid],\n",
    "    valid_names=[\"\"],\n",
    "    # feval=feval,\n",
    "    # fobj=fobj,\n",
    "    # callbacks=[log_evaluation()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"binary\":\n",
    "    res = np.rint(model.predict(lgbtest.data))\n",
    "\n",
    "if TASK == \"multiclass\":\n",
    "    res = model.predict(lgbtest.data).argmax(1)\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"predicted\": res,\n",
    "        \"ground_truth\": df_test[target].values,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     60371\n",
      "           1       0.71      0.75      0.73      3681\n",
      "           2       0.71      0.77      0.74      3597\n",
      "           3       0.34      0.40      0.37      3457\n",
      "           4       0.00      0.00      0.00        19\n",
      "           5       0.95      0.96      0.95      3959\n",
      "           6       0.88      0.51      0.65      6840\n",
      "           7       0.69      0.83      0.75      3318\n",
      "           8       0.35      0.34      0.34      4137\n",
      "           9       0.33      0.37      0.35      3511\n",
      "          10       0.37      0.40      0.38      3736\n",
      "          11       0.84      0.90      0.87      3753\n",
      "          12       0.12      0.15      0.13       109\n",
      "          13       0.79      0.90      0.84      3519\n",
      "\n",
      "    accuracy                           0.84    104007\n",
      "   macro avg       0.58      0.59      0.58    104007\n",
      "weighted avg       0.85      0.84      0.84    104007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\\n{}\".format(classification_report(result[\"predicted\"], result[\"ground_truth\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## w RayTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-08 10:38:03 (running for 00:00:01.22)<br>Memory usage on this node: 3.0/12.2 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.47 GiB heap, 0.0/3.24 GiB objects<br>Result logdir: /home/palo/ray_results/training_function_2021-11-08_10-38-02<br>Number of trials: 2/2 (2 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_91209_00000</td><td>ERROR   </td><td>172.18.71.208:626</td></tr>\n",
       "<tr><td>training_function_91209_00001</td><td>ERROR   </td><td>172.18.71.208:624</td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_91209_00000</td><td style=\"text-align: right;\">           1</td><td>/home/palo/ray_results/training_function_2021-11-08_10-38-02/training_function_91209_00000_0_2021-11-08_10-38-02/error.txt</td></tr>\n",
       "<tr><td>training_function_91209_00001</td><td style=\"text-align: right;\">           1</td><td>/home/palo/ray_results/training_function_2021-11-08_10-38-02/training_function_91209_00001_1_2021-11-08_10-38-02/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [training_function_91209_00000, training_function_91209_00001])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_409/1918576441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m analysis = tune.run(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgbtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlgbvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# resources_per_trial={\"cpu\": 4, \"gpu\": 0},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [training_function_91209_00000, training_function_91209_00001])"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "# config[\"eta\"] = tune.loguniform(1e-4, 1e-1),\n",
    "# config[\"subsample\"] = tune.uniform(0.5, 1.0),\n",
    "config[\"max_depth\"] = (tune.randint(1, 9),)\n",
    "# config[\"wandb\"][\"project\"] = \"GBM_classifier\",\n",
    "# config[\"wandb\"][\"api_key_file\"] = \"../data/wandb_api.key\",\n",
    "# config[\"wandb\"][\"log_config\"] = True\n",
    "\n",
    "\n",
    "def training_function(config, train, valid):\n",
    "    lgbm_config = config.copy()\n",
    "    # lgbm_config.pop(\"wandb\")\n",
    "    trainer = lgbm.train(\n",
    "        lgbm_config,\n",
    "        train,\n",
    "        valid_sets=[valid],\n",
    "        valid_names=[\"\"],\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                {\n",
    "                    ray_metric: ray_metric,\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "asha_scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=ray_metric,\n",
    "    mode=\"min\",\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(training_function, train=lgbtrain, valid=lgbvalid),\n",
    "    # resources_per_trial={\"cpu\": 4, \"gpu\": 0},\n",
    "    num_samples=2,\n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=True),\n",
    "    scheduler=asha_scheduler,\n",
    "    config=config,\n",
    "    # loggers=DEFAULT_LOGGERS + (WandbLogger,),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.trial_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train best params model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = time() - start\n",
    "print(\"Optimization time:\\n{}\".format(runtime))\n",
    "\n",
    "params = copy(analysis.get_best_config(ray_metric, \"min\"))\n",
    "params.pop(\"wandb\")\n",
    "# params[\"n_estimators\"] = 1000\n",
    "\n",
    "start = time()\n",
    "model = lgbm.train(\n",
    "    params,\n",
    "    flgbtrain,\n",
    "    valid_sets=[lgbtest],\n",
    "    callbacks=[lgbm.log_evaluation(show_stdv=False)],\n",
    ")\n",
    "runtime = time() - start\n",
    "print(\"Final model training time:\\n{}\".format(str(datetime.timedelta(seconds=runtime))))a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ~/ray_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
