{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8f2fc2",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This notebook uses preprocessed dataset by following [notebook](12_PowerConverter_dataset_preprocessing.ipynb).\n",
    "\n",
    "**notes**\n",
    "* CPU monitoring in terminal:  \n",
    "```bash\n",
    "top\n",
    "```\n",
    "* GPU monitoring in terminal:  \n",
    "```bash\n",
    "pip install gpustat\n",
    "watch -c gpustat -cp --color\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9e7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# to save results to data directory\n",
    "module_path = \"..\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(1, module_path)\n",
    "# increase displayed columns in jupyter notebook\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c5bbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tracemalloc\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from typing import Union\n",
    "\n",
    "import dill\n",
    "import lightgbm as lgbm\n",
    "import lime\n",
    "import ltv.common as common\n",
    "import ltv.data_postprocessing as data_postprocessing\n",
    "import ltv.data_preprocessing as data_preprocessing\n",
    "import ltv.lightgbm_optimizer as lgbmo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "import sklearn\n",
    "from eli5 import explain_prediction_df, explain_weights, explain_weights_df\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from pytorch_widedeep import Tab2Vec\n",
    "from pytorch_widedeep.utils import LabelEncoder\n",
    "from sklearn.metrics import classification_report, log_loss, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "import tracemalloc\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import JupyterNotebookReporter\n",
    "from ray.tune.integration.lightgbm import TuneReportCheckpointCallback\n",
    "from ray.tune.integration.wandb import WandbLogger\n",
    "from ray.tune.logger import DEFAULT_LOGGERS\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "# temporarily remove deprecation warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1ba16",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71564e",
   "metadata": {},
   "source": [
    "**identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035916cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_types = common.json_load(\"../data/column_types_im.json\")\n",
    "\n",
    "identifier = column_types[\"identifier\"]\n",
    "cat_cols = column_types[\"categorical\"]\n",
    "\n",
    "target = column_types[\"target\"]\n",
    "target_ltv = column_types[\"target_ltv\"]\n",
    "target_ltv_cls = target_ltv + \"_cls\"\n",
    "\n",
    "n_payments = column_types[\"n_payments\"]\n",
    "n_payments_ltv = column_types[\"n_payments_ltv\"]\n",
    "n_payments_cls = n_payments_ltv + \"_cls\"\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "TASK = \"cls\"\n",
    "TEST_SIZE_TRAIN = 0.2\n",
    "TEST_SIZE_VALID = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8a90e",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "* **[PREV]Oversampling of the spenders**\n",
    "```\n",
    "data_reg02_train['temp_cls'] = pd.qcut(data_reg02_train[target_ltv], 15,\n",
    "                                       labels=False, duplicates='drop',\n",
    "                                       retbins=False)\n",
    "#\n",
    "# SCALING OF THE DATA SO IT IS NOT SCALED ON OVERSAMPLED TRAIN\n",
    "#\n",
    "classes = data_reg02_train_scaled['temp_cls'].unique()\n",
    "major_cls_cnt = data_reg02_train_scaled['temp_cls'].value_counts().max()\n",
    "cls_cnts = dict(zip(classes, (np.ones(len(classes))*major_cls_cnt).astype(int)))\n",
    "over = RandomOverSampler(sampling_strategy=cls_cnts, random_state=random_state)\n",
    "data_reg02_train_scaled_over, data_reg02_train_over_labels = over.fit_resample(data_reg02_train_scaled.drop(columns=['temp_cls']),data_reg02_train_scaled['temp_cls'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46632951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle(\n",
    "    \"../scripts/im_pltv_dataset_v1-1_3d_from_2021-02-01_to_2021-08-20_raw.pkl\"\n",
    ")\n",
    "# Fill NA - 0 for numerical and 'NA' for categorical\n",
    "# categorical\n",
    "data_raw[cat_cols].fillna(\"NA\", inplace=True)\n",
    "data_raw[cat_cols] = data_raw[cat_cols].astype(str)\n",
    "# non-categorical\n",
    "non_cat_cols = data_raw.drop(columns=cat_cols + [identifier]).columns.tolist()\n",
    "data_raw[non_cat_cols] = data_raw[non_cat_cols].fillna(0)\n",
    "# drop columns we are not using in evaluation\n",
    "data_raw.drop(columns=[n_payments_ltv, identifier], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7252eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some categorical column names contain \".\", this create an issue when the feature is\n",
    "# one-hot-encodded in pytorch-widedeep, error log:\n",
    "# KeyError: 'module name can\\'t contain \".\", got: emb_layer_battlepass_8008.0'\n",
    "for i, col in enumerate(cat_cols):\n",
    "    if \".\" in col:\n",
    "        cat_cols[i] = col.replace(\".\", \"_\")\n",
    "        data_raw.rename(columns={col: col.replace(\".\", \"_\")}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4672cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset classes before tutorial players drop:\n",
      "0    2343739\n",
      "1      32080\n",
      "Name: sum_payments_package_key_ltv_cls, dtype: int64\n",
      "Dropped constant columns:\n",
      "['key1_get_iap', 'key2_get_iap', 'key3_get_iap', 'diamond_get_iap', 'n_payments_package_key', 'sum_payments_package_key', 'most_frequent_package_key_bought', 'nunique_package_keys_bought', 'battlepass_0_0', 'battlepass_22_0', 'battlepass_23_0', 'battlepass_8004_0', 'battlepass_8005_0', 'battlepass_8008_0', 'time_to_first_purchase', 'time_to_last_purchase', 'time_between_last_purchase_last_login']\n",
      "Fraction of dropped unique categorical feature values:\n",
      "most_frequent_network_type      0.500000\n",
      "most_frequent_iap_bought        0.947368\n",
      "first_time_zone                 0.589744\n",
      "first_login_weekday             0.000000\n",
      "first_device_manufacturer       0.989431\n",
      "first_device_model              0.873106\n",
      "most_frequent_country           0.740260\n",
      "first_login_day_time            0.000000\n",
      "first_login_country             0.739130\n",
      "form_factor                     0.400000\n",
      "is_paid_user                    0.333333\n",
      "first_device_os                 0.000000\n",
      "device_language                 0.863636\n",
      "first_network_type              0.428571\n",
      "first_login_country_is_mfreq    0.000000\n",
      "dtype: float64\n",
      "Size of training dataset classes after tutorial players drop:\n",
      "0    1874991\n",
      "1      25664\n",
      "Name: sum_payments_package_key_ltv_cls, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "PowerTran = False\n",
    "\n",
    "if TASK == \"cls\":\n",
    "    data = data_raw[data_raw[target] == 0].reset_index(drop=True).copy()\n",
    "    data[target_ltv_cls] = (data[target_ltv] > 0).astype(int)\n",
    "    print(\n",
    "        \"Size of dataset classes before tutorial players drop:\\n{}\".format(\n",
    "            data[target_ltv_cls].value_counts()\n",
    "        )\n",
    "    )\n",
    "    target_col = target_ltv_cls\n",
    "if TASK == \"reg\":\n",
    "    # regressor02 - data\n",
    "    data = data_raw[data_raw[target] > 0].reset_index(drop=True).copy()\n",
    "    # regressor01 - data\n",
    "    # data = data_raw[data_raw[target] == 0 and data_raw[target_ltv] > 0].reset_index(drop=True).copy()\n",
    "    data[target_ltv] = data[target_ltv] - data[target]\n",
    "    print(\n",
    "        \"Size of dataset\\n\\u2022 fraction of all data: {}\\n\\u2022 number of samples: {}\".format(\n",
    "            len(data) / len(data_raw), len(data)\n",
    "        )\n",
    "    )\n",
    "    target_col = target_ltv\n",
    "\n",
    "data, cat_cols_f, cont_cols_f = data_preprocessing.cols_preprocess(\n",
    "    data, cat_cols, target_col, task=TASK, verbose=True\n",
    ")\n",
    "\n",
    "# train,test,valid\n",
    "if TASK == \"cls\":\n",
    "    data_train, data_valid = train_test_split(\n",
    "        data,\n",
    "        test_size=TEST_SIZE_TRAIN,\n",
    "        stratify=data[target_ltv_cls],\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    data_valid, data_test = train_test_split(\n",
    "        data_valid,\n",
    "        test_size=TEST_SIZE_VALID,\n",
    "        stratify=data_valid[target_ltv_cls],\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "if TASK == \"reg\":\n",
    "    data_train, data_valid = train_test_split(\n",
    "        data, test_size=TEST_SIZE_TRAIN, random_state=RANDOM_STATE\n",
    "    )\n",
    "    data_valid, data_test = train_test_split(\n",
    "        data_valid, test_size=TEST_SIZE_VALID, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "data_valid.reset_index(inplace=True, drop=True)\n",
    "data_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# data scale\n",
    "data_train_scaled, Scaler = data_preprocessing.scale(\n",
    "    data_train, cat_cols_f + [target_col], scaler_sk=\"Standard\"\n",
    ")\n",
    "data_valid_scaled, Scaler = data_preprocessing.scale(\n",
    "    data_valid, cat_cols_f + [target_col], scaler_sk=Scaler\n",
    ")\n",
    "data_test_scaled, Scaler = data_preprocessing.scale(\n",
    "    data_test, cat_cols_f + [target_col], scaler_sk=Scaler\n",
    ")\n",
    "\n",
    "# regressor power transform of target_ltv\n",
    "if TASK == \"reg\" and PowerTran == True:\n",
    "    Ptran = PowerTransformer(standardize=False)\n",
    "    Ptran.fit(\n",
    "        data_train_scaled.loc[\n",
    "            data_train_scaled[target_ltv] > 0, target_ltv\n",
    "        ].values.reshape(-1, 1)\n",
    "    )\n",
    "    data_train_scaled.loc[\n",
    "        data_train_scaled[target_ltv] > 0, target_ltv\n",
    "    ] = Ptran.transform(\n",
    "        data_train_scaled.loc[\n",
    "            data_train_scaled[target_ltv] > 0, target_ltv\n",
    "        ].values.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    data_valid_scaled.loc[\n",
    "        data_valid_scaled[target_ltv] > 0, target_ltv\n",
    "    ] = Ptran.transform(\n",
    "        data_valid_scaled.loc[\n",
    "            data_valid_scaled[target_ltv] > 0, target_ltv\n",
    "        ].values.reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    # no need to power-transform test target_ltv\n",
    "    # data_reg_test_scaled.loc[data_reg_train_scaled[target_ltv] > 0, target_ltv] = Ptran_reg.transform(data_reg_train_scaled. loc[data_reg_train_scaled[target_ltv] > 0,target_ltv].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Drop tutorial players in the train dataset + [CHECK] Dataset imbalance\n",
    "if TASK == \"cls\":\n",
    "    feature_to_drop = \"n_ads_watched\"\n",
    "    data_train_scaled = data_train_scaled[\n",
    "        data_train_scaled[feature_to_drop] != 0\n",
    "    ].reset_index(drop=True)\n",
    "    print(\n",
    "        \"Size of training dataset classes after tutorial players drop:\\n{}\".format(\n",
    "            data_train[target_ltv_cls].value_counts()\n",
    "        )\n",
    "    )\n",
    "    data_train_scaled.drop(columns=[target_ltv], inplace=True)\n",
    "    data_valid_scaled.drop(columns=[target_ltv], inplace=True)\n",
    "    data_test_scaled.drop(columns=[target_ltv], inplace=True)\n",
    "    cont_cols_f.remove(target_ltv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9779635",
   "metadata": {},
   "source": [
    "**[PLACEHOLDER]Create classes ZERO-REPEAT vs NON-ZERO-REPEAT vs NON-SPENDER**\n",
    "* 0 - non-spenders\n",
    "* 1 - zero-repeat spender\n",
    "* 2 - non-zero-repeat spender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd7e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clsZNZ = data_raw[data_raw[n_payments] > 1].reset_index(drop=True).copy()\n",
    "# data_clsZNZ[n_payments_cls] = 0\n",
    "# data_clsZNZ.loc[data_clsZNZ[n_payments_ltv] == 1, n_payments_cls] = 1\n",
    "# data_clsZNZ.loc[data_clsZNZ[n_payments_ltv] > 1, n_payments_cls] = 2\n",
    "# data_clsZNZ.drop(columns=[target_ltv, n_payments_ltv, identifier], inplace=True)\n",
    "# data_clsZNZ[n_payments_cls].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087031f",
   "metadata": {},
   "source": [
    "## Categorical features tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db80212",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_FEATURE_TRANSFORMATION = \"Entity Embedding\"\n",
    "\n",
    "if CAT_FEATURE_TRANSFORMATION == \"Label Encoding\":\n",
    "    label_encoder = LabelEncoder(cat_cols_f)\n",
    "    label_encoder.fit(data[cat_cols_f])\n",
    "\n",
    "    data_train_scaled_enc = data_train_scaled.copy()\n",
    "    data_valid_scaled_enc = data_valid_scaled.copy()\n",
    "    data_test_scaled_enc = data_test_scaled.copy()\n",
    "\n",
    "    data_train_scaled_enc[cat_cols_f] = label_encoder.transform(\n",
    "        data_train_scaled_enc[cat_cols_f]\n",
    "    )\n",
    "    data_valid_scaled_enc[cat_cols_f] = label_encoder.transform(\n",
    "        data_valid_scaled_enc[cat_cols_f]\n",
    "    )\n",
    "    data_test_scaled_enc[cat_cols_f] = label_encoder.transform(\n",
    "        data_test_scaled_enc[cat_cols_f]\n",
    "    )\n",
    "    data_test_scaled_enc[cat_cols_f].head()\n",
    "\n",
    "if CAT_FEATURE_TRANSFORMATION == \"Entity Embedding\":\n",
    "    # using pretrained embedding from pytorch-widedeep model and its tab_preprocessor\n",
    "    with open(\"dl_entity_emb_model_\" + TASK + \".dill\", \"rb\") as f:\n",
    "        model = dill.load(f)\n",
    "    with open(\"dl_entity_emb_model_tab_preprocessor_\" + TASK + \".dill\", \"rb\") as f:\n",
    "        tab_preprocessor = dill.load(f)\n",
    "\n",
    "    t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor, return_dataframe=True)\n",
    "    data_train_scaled_enc, data_train_y = t2v.transform(\n",
    "        data_train_scaled, target_col=target_col\n",
    "    )\n",
    "    data_valid_scaled_enc, data_valid_y = t2v.transform(\n",
    "        data_valid_scaled, target_col=target_col\n",
    "    )\n",
    "    data_test_scaled_enc, data_test_y = t2v.transform(\n",
    "        data_test_scaled, target_col=target_col\n",
    "    )\n",
    "    data_train_scaled_enc[target_col] = data_train_y\n",
    "    data_valid_scaled_enc[target_col] = data_valid_y\n",
    "    data_test_scaled_enc[target_col] = data_test_y\n",
    "\n",
    "    cols_list = list(data_test_scaled_enc.columns)\n",
    "    cat_cols_f_emb = []\n",
    "    for cat_col in cat_cols_f:\n",
    "        r = re.compile(cat_col + \"*\")\n",
    "        cat_cols_f_emb.extend(list(filter(r.match, cols_list)))\n",
    "# data_test_scaled_enc[cat_cols_f_emb].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393a56a",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080ac7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled_enc = data_train_scaled_enc.sample(100000)\n",
    "data_valid_scaled_enc = data_valid_scaled_enc.sample(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1720537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tracemalloc.take_snapshot == \"cls\":\n",
    "    # config[\"objective\"] = \"binary\"\n",
    "    # config[\"metric\"] = \"binary_logloss\"\n",
    "    # focal_loss https://github.com/jrzaurin/LightGBM-with-Focal-Loss\n",
    "    config[\"is_unbalance\"] = True\n",
    "    fobj = focal_loss = lambda x, y: focal_loss_lgb(x, y, 0.25, 1.0)\n",
    "    feval = eval_error = lambda x, y: focal_loss_lgb_eval_error(x, y, 0.25, 1.0)\n",
    "    ray_metric = \"-\" + \"focal_loss\"\n",
    "\n",
    "if TASK == \"reg\":\n",
    "    config[\"objective\"] = \"regression\"\n",
    "    config[\"metric\"] = \"rmse\"\n",
    "    fobj = None\n",
    "    feval = None\n",
    "    ray_metric = \"-\" + config[\"metric\"]\n",
    "\n",
    "if CAT_FEATURE_TRANSFORMATION == \"Label Encoding\":\n",
    "    lgb_cat_cols = cat_cols_f\n",
    "if CAT_FEATURE_TRANSFORMATION == \"Entity Embedding\":\n",
    "    lgb_cat_cols = []\n",
    "\n",
    "lgbtrain = lgbm.Dataset(\n",
    "    data_train_scaled_enc.drop(columns=[target_col]),\n",
    "    data_train_scaled_enc[target_col],\n",
    "    categorical_feature=lgb_cat_cols,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "lgbvalid = lgbm.Dataset(\n",
    "    data_valid_scaled_enc.drop(columns=[target_col]),\n",
    "    data_valid_scaled_enc[target_col],\n",
    "    reference=lgbtrain,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "# Final TRAIN/TEST\n",
    "ftrain = pd.concat([data_train_scaled_enc, data_valid_scaled_enc]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "flgbtrain = lgbm.Dataset(\n",
    "    ftrain.drop(columns=[target_col]),\n",
    "    ftrain[target_col],\n",
    "    categorical_feature=lgb_cat_cols,\n",
    "    free_raw_data=False,\n",
    ")\n",
    "lgbtest = lgbm.Dataset(\n",
    "    data_test_scaled_enc.drop(columns=[target_col]),\n",
    "    data_test_scaled_enc[target_col],\n",
    "    categorical_feature=lgb_cat_cols,\n",
    "    reference=flgbtrain,\n",
    "    free_raw_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b92bceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.9/30.6 GiB<br>Using AsyncHyperBand: num_stopped=2\n",
       "Bracket: Iter 90.000: -0.20640595568683023 | Iter 30.000: -0.2389568821798404 | Iter 10.000: -0.2518617562197316<br>Resources requested: 0/7 CPUs, 0/0 GPUs, 0.0/10.96 GiB heap, 0.0/5.48 GiB objects<br>Result logdir: /home/jovyan/ray_results/training_function_2021-10-25_13-57-38<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">       eta</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  subsample</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  -focal_loss</th><th style=\"text-align: right;\">   -f1_0</th><th style=\"text-align: right;\">  -f1_1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_8361f_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00186841</td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">   0.572318</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        36.2122 </td><td style=\"text-align: right;\">     0.201523</td><td style=\"text-align: right;\">0.993289</td><td style=\"text-align: right;\">      0</td></tr>\n",
       "<tr><td>training_function_8361f_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.00128785</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">   0.828295</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         8.01187</td><td style=\"text-align: right;\">     0.253181</td><td style=\"text-align: right;\">0.993289</td><td style=\"text-align: right;\">      0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-25 13:58:19,895\tINFO tune.py:617 -- Total run time: 41.70 seconds (41.51 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization time:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "config = {\n",
    "    \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"max_depth\": tune.randint(1, 9),\n",
    "    \"wandb\": {\n",
    "        \"project\": \"GBM_classifier\",\n",
    "        \"api_key_file\": \"../data/wandb_api.key\",\n",
    "        \"log_config\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def training_function(config, train, valid):\n",
    "    lgbm_config = config.copy()\n",
    "    lgbm_config.pop(\"wandb\")\n",
    "    trainer = lgbm.train(\n",
    "        lgbm_config,\n",
    "        train,\n",
    "        valid_sets=[valid],\n",
    "        valid_names=[\"\"],\n",
    "        verbose_eval=False,\n",
    "        feval=[\n",
    "            lgbmo.feval,\n",
    "            lgbmo.lgb_focal_f1_1,\n",
    "            lgbmo.lgb_focal_f1_0,\n",
    "            lgbmo.lgb_focal_recall_0,\n",
    "            lgbmo.lgb_focal_recall_1,\n",
    "            lgbmo.lgb_focal_precision_0,\n",
    "            lgbmo.lgb_focal_precision_1,\n",
    "            lgbmo.lgb_focal_accuracy,\n",
    "        ],\n",
    "        fobj=fobj,\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                {\n",
    "                    ray_metric: ray_metric,\n",
    "                    \"-f1_0\": \"-f1_0\",\n",
    "                    \"-f1_1\": \"-f1_1\",\n",
    "                    \"-recall_0\": \"-recall_0\",\n",
    "                    \"-recall_1\": \"-recall_1\",\n",
    "                    \"-precision_0\": \"-precision_0\",\n",
    "                    \"-precision_1\": \"-precision_1\",\n",
    "                    \"-accuracy\": \"-accuracy\",\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "asha_scheduler = AsyncHyperBandScheduler(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=ray_metric,\n",
    "    mode=\"min\",\n",
    "    max_t=100,\n",
    "    grace_period=10,\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(training_function, train=lgbtrain, valid=lgbvalid),\n",
    "    # resources_per_trial={\"cpu\": 4, \"gpu\": 0},\n",
    "    num_samples=2,\n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=True),\n",
    "    scheduler=asha_scheduler,\n",
    "    config=config,\n",
    "    loggers=DEFAULT_LOGGERS + (WandbLogger,),\n",
    ")\n",
    "\n",
    "runtime = time() - start\n",
    "print(\"Optimization time:\\n\".format(runtime))\n",
    "\n",
    "params = copy(analysis.get_best_config(ray_metric, \"min\"))\n",
    "params.pop(\"wandb\")\n",
    "# params[\"n_estimators\"] = 1000\n",
    "\n",
    "start = time()\n",
    "model = lgbm.train(\n",
    "    params,\n",
    "    flgbtrain,\n",
    "    valid_sets=[lgbtest],\n",
    "    callbacks=[lgbm.log_evaluation(show_stdv=False)],\n",
    ")\n",
    "runtime = time() - start\n",
    "print(\"Final model training time:\\n{}\".format(str(datetime.timedelta(seconds=runtime))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cd78958",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.trial_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306341f8",
   "metadata": {},
   "source": [
    "## Prediction & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ad94926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    234374\n",
      "           1       0.33      0.01      0.02      3208\n",
      "\n",
      "    accuracy                           0.99    237582\n",
      "   macro avg       0.66      0.51      0.51    237582\n",
      "weighted avg       0.98      0.99      0.98    237582\n",
      "\n",
      "Missed summed LTV fraction in target_ltv==0 dataset as of incorrect classification:\n",
      "0.9845011019458371\n",
      "Summed LTV in target_ltv==0 dataset:\n",
      "87082.31999999999\n"
     ]
    }
   ],
   "source": [
    "# the classifier predictions do not return labels but value beween 0 and 1 ?\n",
    "result = pd.DataFrame(\n",
    "    {\"PLTV\": np.rint(model.predict(lgbtest.data)), \"LTV\": data_test[target_ltv].values}\n",
    ")\n",
    "\n",
    "if TASK == \"cls\":\n",
    "    print(\n",
    "        \"Classification report:\\n{}\".format(\n",
    "            classification_report((result[\"LTV\"] > 0).astype(int), result[\"PLTV\"])\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Missed summed LTV fraction in target_ltv==0 dataset as of incorrect classification:\\n{}\".format(\n",
    "            result[(result[\"LTV\"] > 0) & (result[\"PLTV\"] == 0)][\"LTV\"].sum()\n",
    "            / result[\"LTV\"].sum()\n",
    "        )\n",
    "    )\n",
    "    print(\"Summed LTV in target_ltv==0 dataset:\\n{}\".format(result[\"LTV\"].sum()))\n",
    "if TASK == \"reg\":\n",
    "    if PowerTran == True:\n",
    "        result[\"PLTV\"] = Ptran.inverse_transform(\n",
    "            result[\"PLTV\"].to_numpy().reshape(-1, 1)\n",
    "        )\n",
    "    print(\n",
    "        \"RMSE score:\\n{}\".format(\n",
    "            str(mean_squared_error(result[\"LTV\"], result[\"PLTV\"], squared=False))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587705ff",
   "metadata": {},
   "source": [
    "# APPENDIX - Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ab343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ~/ray_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
