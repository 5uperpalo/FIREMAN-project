{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream-based machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE: \n",
    "* Concept drift algorithms\n",
    "* Hoeffding Tree\n",
    "* Hoeffding Adaptive Tree\n",
    "* Evaluation - Holdout, Prequential, \"Real-world\" (incremetal)\n",
    "* Note : same functionality can be achieved with higher performance in java based MOA (skmultiflow is a \"child project\" to MOA )\n",
    "\n",
    "### Concept drift\n",
    "\n",
    "Concept drifts categories:\n",
    "<img src=\"./images/concept_drifts.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "    [1]\n",
    "#### ADWIN\n",
    "<img src=\"./images/adwin.png\" alt=\"drawing\" width=\"500\"/>\n",
    "    \n",
    "    [2-3]\n",
    "\n",
    "<small><small><small>\n",
    "    \n",
    "    [1] Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A. (2014). \"A survey on concept drift adaptation.\" ACM computing surveys (CSUR), 46(4), 1-37.\n",
    "\n",
    "    [2] Grulich, Philipp Marian, et al. \"Scalable Detection of Concept Drifts on Data Streams with Parallel Adaptive Windowing.\" EDBT. 2018.\n",
    "\n",
    "    [3] Bifet, Albert, and Ricard Gavalda. \"Learning from time-changing data with adaptive windowing.\" Proceedings of the 2007 SIAM international conference on data mining. Society for Industrial and Applied Mathematics, 2007.\n",
    "</small></small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#https://scikit-multiflow.github.io/scikit-multiflow/documentation.html#learning-methods\n",
    "from skmultiflow.drift_detection import DDM\n",
    "from skmultiflow.drift_detection.eddm import EDDM\n",
    "from skmultiflow.drift_detection import PageHinkley\n",
    "from skmultiflow.drift_detection.adwin import ADWIN\n",
    "from skmultiflow.evaluation import EvaluateHoldout\n",
    "\n",
    "from skmultiflow.meta import AdaptiveRandomForest\n",
    "from skmultiflow.trees import HoeffdingTree\n",
    "from skmultiflow.trees import HAT\n",
    "from skmultiflow.evaluation import EvaluatePrequential\n",
    "\n",
    "from skmultiflow.data import DataStream\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataset):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(dataset.drop(columns=['fault_id']))\n",
    "    dataset_scaled = pd.DataFrame(scaler.transform(dataset.drop(columns=['fault_id'])),columns=col_names[:-1])\n",
    "    dataset_scaled['fault_id'] = dataset['fault_id'].copy()\n",
    "    return dataset_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('workshop_dataset.csv',index_col=False).sample(frac=1).reset_index(drop=True)\n",
    "col_names = dataset.columns.tolist()\n",
    "test_size = len(dataset)//10\n",
    "training_size = len(dataset) - test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Concept Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnitude of row vectors - concept drift detectors take as input single value not list/vector\n",
    "data_stream_magnitude = dataset[dataset.columns[:-1]].apply(np.linalg.norm, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change detected in data: 7644.600911861115 - at index: 28\n",
      "Change detected in data: 7619.368761792521 - at index: 57\n"
     ]
    }
   ],
   "source": [
    "adwin = ADWIN()\n",
    "ddm = DDM()\n",
    "eddm = EDDM()\n",
    "ph = PageHinkley()\n",
    "\n",
    "adwin_detected_changes = []\n",
    "ddm_detected_changes = []\n",
    "eddm_detected_changes = []\n",
    "ph_detected_changes = []\n",
    "\n",
    "for i in range(len(data_stream_magnitude)):\n",
    "    adwin.add_element(data_stream[i])\n",
    "    ddm.add_element(data_stream[i])\n",
    "    eddm.add_element(data_stream[i])\n",
    "    ph.add_element(data_stream[i])\n",
    "    if adwin.detected_change():\n",
    "        adwin_detected_changes.extend(i)\n",
    "    if ddm.detected_change():\n",
    "        ddm_detected_changes.extend(i)\n",
    "    if eddm.detected_change():\n",
    "        eddm_detected_changes.extend(i)\n",
    "    if ph.detected_change():\n",
    "        ph_detected_changes.extend(i)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification pipeline\n",
    "\n",
    "2.1 Holdout - follows batch machine learning logic, i.e. train incremetanlly models and test it on test dataset  \n",
    "2.2 Prequential - test-then-train  \n",
    "2.3 Real-world scenarios - model is incrementally trained and then incremetally tested by comparing predicted labels to ground truth  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Holdout evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HT.reset()\n",
    "HAT.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Evaluation\n",
      "Evaluating 1 target(s).\n",
      "Separating 2023 holdout samples.\n",
      "Evaluating...\n",
      "\n",
      "Processed samples: 11723\n",
      "Mean performance:\n",
      "HT - Precision: 1.0000\n",
      "HT - Recall: 1.0000\n",
      "HT - F1 score: 1.0000\n",
      "HAT - Precision: 1.0000\n",
      "HAT - Recall: 0.9564\n",
      "HAT - F1 score: 0.9777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HoeffdingTree(binary_split=False, grace_period=200, leaf_prediction='nba',\n",
       "               max_byte_size=33554432, memory_estimate_period=1000000,\n",
       "               nb_threshold=0, no_preprune=False, nominal_attributes=None,\n",
       "               remove_poor_atts=None, split_confidence=1e-07,\n",
       "               split_criterion='info_gain', stop_mem_management=False,\n",
       "               tie_threshold=0.05),\n",
       " HAT(binary_split=False, grace_period=200, leaf_prediction='nba',\n",
       "     max_byte_size=33554432, memory_estimate_period=1000000, nb_threshold=0,\n",
       "     no_preprune=False, nominal_attributes=None, remove_poor_atts=False,\n",
       "     split_confidence=1e-07, split_criterion='info_gain',\n",
       "     stop_mem_management=False, tie_threshold=0.05)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = dataset.drop(columns=['fault_id'])\n",
    "labels  = dataset['fault_id'].to_frame()\n",
    "\n",
    "stream = DataStream(data = samples, y = labels)\n",
    "stream.prepare_for_use()\n",
    "\n",
    "#HT = HoeffdingTree()\n",
    "#HAT = HAT()\n",
    "#ARF = AdaptiveRandomForest()\n",
    "evaluator = EvaluateHoldout(max_samples=100000,\n",
    "                            max_time=6000,\n",
    "                            n_wait=100,                            \n",
    "                            batch_size=100,\n",
    "                            test_size=test_size,\n",
    "                            output_file='HAT_houldout.csv',\n",
    "                            metrics=['precision','recall','f1'])\n",
    "evaluator.evaluate(stream=stream, model=[HT,HAT], model_names=['HT','HAT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Prequential evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "HT.reset()\n",
    "HAT.reset()\n",
    "evaluator = EvaluatePrequential(n_wait=100, \n",
    "                                batch_size=100, \n",
    "                                pretrain_size=training_size, \n",
    "                                output_file='HAT_prequential.csv',\n",
    "                                metrics=['precision','recall','f1'])\n",
    "evaluator.evaluate(stream=stream, model=HAT, model_names=['HAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skmultiflow saves results to file with leading 5 lines containing configuraiton of evaluation, learner etc\n",
    "# skmultiflow also did not evaluate last 200 samples\n",
    "# for the sake of comparisson we shrink the MOA results\n",
    "# accuracy in MOA is in % and in skmultiflow fraction\n",
    "HAT_results = pd.read_csv('HAT_prequential.csv',skiprows=[0,1,2,3,4],index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HAT_results[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Real-world scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Incremental approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAT.reset()\n",
    "\n",
    "samples_train = dataset.drop(columns=['fault_id']).values[:train_size]\n",
    "labels_train  = dataset['fault_id'].to_frame().values.flatten()[:train_size]\n",
    "\n",
    "stream_train = DataStream(data=samples_train, y=labels_train)\n",
    "stream_train.prepare_for_use()\n",
    "\n",
    "for sample in range(len(labels_train)):\n",
    "    X, Y = stream_train.next_sample()\n",
    "    HAT.partial_fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/big-dama/anaconda3/lib/python3.7/site-packages/skmultiflow/metrics/measure_collection.py:138: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return sum_value / self.sample_count\n"
     ]
    }
   ],
   "source": [
    "samples_test = dataset_test.drop(columns=['fault_id']).values[train_size:]\n",
    "labels_test  = dataset_test['fault_id'].to_frame().values.flatten()[train_size:]\n",
    "\n",
    "stream_test = DataStream(data = samples_test, y = labels_test)\n",
    "stream_test.prepare_for_use()\n",
    "\n",
    "labels_test_predicted = []\n",
    "for sample in range(len(labels_test)):\n",
    "    X, Y = stream_test.next_sample()\n",
    "    Y_pred = HAT.predict(X)\n",
    "    labels_test_predicted.extend(HAT.predict(X))\n",
    "    \n",
    "print('Classification report :\\n' + str(classification_report(labels_test, labels_test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Bulk approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveRandomForest(binary_split=False, disable_weighted_vote=False,\n",
       "                     drift_detection_method=ADWIN(delta=0.001), grace_period=50,\n",
       "                     lambda_value=6, leaf_prediction='nba',\n",
       "                     max_byte_size=33554432, max_features=1,\n",
       "                     memory_estimate_period=2000000, n_estimators=10,\n",
       "                     nb_threshold=0, no_preprune=False, nominal_attributes=None,\n",
       "                     performance_metric='acc', random_state=None,\n",
       "                     remove_poor_atts=False, split_confidence=0.01,\n",
       "                     split_criterion='info_gain', stop_mem_management=False,\n",
       "                     tie_threshold=0.05,\n",
       "                     warning_detection_method=ADWIN(delta=0.01))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAT.reset()\n",
    "HAT.fit(samples_train,labels_train.flatten())\n",
    "labels_test_predicted = ARF.predict(samples_test)\n",
    "print('Classification report :\\n' + str(classification_report(labels_test, labels_test_predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
